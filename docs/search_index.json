[
["index.html", "Data analysis and R programming Chapter 1 Introduction 1.1 Installation 1.2 Programme 1.3 License and credits", " Data analysis and R programming Laurent Gatto 2017-11-05 Chapter 1 Introduction This course presents a 2-day introduction to data analysis and visualisation with R as well as certain programming topics. It is formatted as a bookdown document The material can be accessed here: https://lgatto.github.io/2017_11_09_Rcourse_Jena/ The source code for this document is available on GitHub at https://github.com/lgatto/2017_11_09_Rcourse_Jena. 1.1 Installation We will install required packages as we go along. Before the course, participants should install a recent version of R (ideally the current release version) and RStudio, or any other editor of their choice. 1.2 Programme Morning: 9:30 - 13:00 (3 hours, with 30 min break) Afternoon: 14:00 - 17:00 (3 hours, with a shorter break) Day 1 Setup and introduction Spreadsheets and tidy data Data manipulation using the tidyverse Visualisation with ggplot2 Reproducible research with Rmarkdown Wrap-up Day 2 R package development Viz. of high-dimensional data (bit.ly/highdimvis) Interactive visualisation: ggvis, shiny Navigate and mine Bioconductor for useful scripting components Wrap-up Optional R programming concepts and tools 1.3 License and credits This material is licensed under the Creative Commons Attribution-ShareAlike 3.0 License. Some content is inspired by other sources, see the first paragraph in the respective chapters for credits. "],
["tidy-data.html", "Chapter 2 Tidy data 2.1 Introduction 2.2 Common Spreadsheet Errors 2.3 Key points", " Chapter 2 Tidy data This material is based on the Data Carpentry Data Organization in Spreadsheets lesson. 2.1 Introduction However, most time is spend in clearning up and preparing the data, rather than actually analysing it. Like families, tidy datasets are all alike but every messy dataset is messy in its own way - (Hadley Wickham) An example of a really messy data! 2.2 Common Spreadsheet Errors There are a few potential errors to be on the lookout for in your own data as well as data from collaborators or the Internet. If you are aware of the errors and the possible negative effect on downstream data analysis and result interpretation, it might motivate yourself and your project members to try and avoid them. Making small changes to the way you format your data in spreadsheets, can have a great impact on efficiency and reliability when it comes to data cleaning and analysis. Using multiple tables Using multiple tabs Not filling in zeros Using problematic null values Using formatting to convey information Using formatting to make the data sheet look pretty Placing comments or units in cells Entering more than one piece of information in a cell Using problematic field names Using special characters in data Inclusion of metadata in data table Date formatting Using multiple tables A common strategy is creating multiple data tables within one spreadsheet. This confuses the computer, so don’t do this! When you create multiple tables within one spreadsheet, you’re drawing false associations between things for the computer, which sees each row as an observation. You’re also potentially using the same field name in multiple places, which will make it harder to clean your data up into a usable form. The example below depicts the problem: multiple tabs In the example above, the computer will see (for example) row 4 and assume that all columns A-AF refer to the same sample. This row actually represents four distinct samples (sample 1 for each of four different collection dates - May 29th, June 12th, June 19th, and June 26th), as well as some calculated summary statistics (an average (avr) and standard error of measurement (SEM)) for two of those samples. Other rows are similarly problematic. Using multiple tabs But what about workbook tabs? That seems like an easy way to organize data, right? Well, yes and no. When you create extra tabs, you fail to allow the computer to see connections in the data that are there (you have to introduce spreadsheet application-specific functions or scripting to ensure this connection). Say, for instance, you make a separate tab for each day you take a measurement. This isn’t good practice for two reasons: you are more likely to accidentally add inconsistencies to your data if each time you take a measurement, you start recording data in a new tab, and even if you manage to prevent all inconsistencies from creeping in, you will add an extra step for yourself before you analyze the data because you will have to combine these data into a single datatable. You will have to explicitly tell the computer how to combine tabs - and if the tabs are inconsistently formatted, you might even have to do it manually. The next time you’re entering data, and you go to create another tab or table, ask yourself if you could avoid adding this tab by adding another column to your original spreadsheet. Your data sheet might get very long over the course of the experiment. This makes it harder to enter data if you can’t see your headers at the tnop of the spreadsheet. But don’t repeat your header row. These can easily get mixed into the data, leading to problems down the road. Instead you can freeze the column headers so that they remain visible even when you have a spreadsheet with many rows. Not filling in zeros It might be that when you’re measuring something, it’s usually a zero, say the number of times a rabbit is observed in the survey. Why bother writing in the number zero in that column, when it’s mostly zeros? However, there’s a difference between a zero and a blank cell in a spreadsheet. To the computer, a zero is actually data. You measured or counted it. A blank cell means that it wasn’t measured and the computer will interpret it as an unknown value (otherwise known as a null value). The spreadsheets or statistical programs will likely mis-interpret blank cells that you intend to be zeros. By not entering the value of your observation, you are telling your computer to represent that data as unknown or missing (null). This can cause problems with subsequent calculations or analyses. For example, the average of a set of numbers which includes a single null value is always null (because the computer can’t guess the value of the missing observations). Because of this, it’s very important to record zeros as zeros and truly missing data as nulls. Using problematic null values Example: using -999 or other numerical values (or zero) to represent missing data. Solution: One common practice is to record unknown or missing data as -999, 999, or 0. Many statistical programs will not recognize that these are intended to represent missing (null) values. How these values are interpreted will depend on the software you use to analyze your data. It is essential to use a clearly defined and consistent null indicator. Blanks (most applications) and NA (for R) are good choices. White et al, 2013, explain good choices for indicating null values for different software applications in their article: Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution. White et al. Using formatting to convey information Example: highlighting cells, rows or columns that should be excluded from an analysis, leaving blank rows to indicate separations in data. formatting Solution: create a new field to encode which data should be excluded. good formatting Using formatting to make the data sheet look pretty Example: merging cells. Solution: If you’re not careful, formatting a worksheet to be more aesthetically pleasing can compromise your computer’s ability to see associations in the data. Merged cells will make your data unreadable by statistics software. Consider restructuring your data in such a way that you will not need to merge cells to organize your data. Placing comments or units in cells Example: Your data was collected, in part, by a summer student who you later found out was mis-identifying some of your species, some of the time. You want a way to note these data are suspect. Solution: Most analysis software can’t see Excel or LibreOffice comments, and would be confused by comments placed within your data cells. As described above for formatting, create another field if you need to add notes to cells. Similarly, don’t include units in cells: ideally, all the measurements you place in one column should be in the same unit, but if for some reason they aren’t, create another field and specify the units the cell is in. Entering more than one piece of information in a cell Example: You find one male, and one female of the same species. You enter this as 1M, 1F. Solution: Don’t include more than one piece of information in a cell. This will limit the ways in which you can analyze your data. If you need both these measurements, design your data sheet to include this information. For example, include one column for number of individuals and a separate column for sex. Using problematic field names Choose descriptive field names, but be careful not to include spaces, numbers, or special characters of any kind. Spaces can be misinterpreted by parsers that use whitespace as delimiters and some programs don’t like field names that are text strings that start with numbers. Underscores (_) are a good alternative to spaces. Consider writing names in camel case (like this: ExampleFileName) to improve readability. Remember that abbreviations that make sense at the moment may not be so obvious in 6 months, but don’t overdo it with names that are excessively long. Including the units in the field names avoids confusion and enables others to readily interpret your fields. Examples Using special characters in data Example: You treat your spreadsheet program as a word processor when writing notes, for example copying data directly from Word or other applications. Solution: This is a common strategy. For example, when writing longer text in a cell, people often include line breaks, em-dashes, etc in their spreadsheet. Also, when copying data in from applications such as Word, formatting and fancy non-standard characters (such as left- and right-aligned quotation marks) are included. When exporting this data into a coding/statistical environment or into a relational database, dangerous things may occur, such as lines being cut in half and encoding errors being thrown. General best practice is to avoid adding characters such as newlines, tabs, and vertical tabs. In other words, treat a text cell as if it were a simple web form that can only contain text and spaces. Inclusion of metadata in data table Example: You add a legend at the top or bottom of your data table explaining column meaning, units, exceptions, etc. Solution: Recording data about your data (“metadata”) is essential. You may be on intimate terms with your dataset while you are collecting and analysing it, but the chances that you will still remember that the variable “sglmemgp” means single member of group, for example, or the exact algorithm you used to transform a variable or create a derived one, after a few months, a year, or more are slim. As well, there are many reasons other people may want to examine or use your data - to understand your findings, to verify your findings, to review your submitted publication, to replicate your results, to design a similar study, or even to archive your data for access and re-use by others. While digital data by definition are machine-readable, understanding their meaning is a job for human beings. The importance of documenting your data during the collection and analysis phase of your research cannot be overestimated, especially if your research is going to be part of the scholarly record. However, metadata should not be contained in the data file itself. Unlike a table in a paper or a supplemental file, metadata (in the form of legends) should not be included in a data file since this information is not data, and including it can disrupt how computer programs interpret your data file. Rather, metadata should be stored as a separate file in the same directory as your data file, preferably in plain text format with a name that clearly associates it with your data file. Because metadata files are free text format, they also allow you to encode comments, units, information about how null values are encoded, etc. that are important to document but can disrupt the formatting of your data file. Additionally, file or database level metadata describes how files that make up the dataset relate to each other; what format are they are in; and whether they supercede or are superceded by previous files. A folder-level readme.txt file is the classic way of accounting for all the files and folders in a project. 2.3 Key points Avoid using multiple tables within one spreadsheet. Avoid spreading data across multiple tabs (but do use a new tab to record data cleaning or manipulations). Record zeros as zeros. Use an appropriate null value to record missing data. Don’t use formatting to convey information or to make your spreadsheet look pretty. Place comments in a separate column. Record units in column headers. Include only one piece of information in a cell. Avoid spaces, numbers and special characters in column headers. Avoid special characters in your data. Record metadata in a separate plain text file. An excellent reference, in particular with regard to R scripting is Hadley Wickham, Tidy Data, Vol. 59, Issue 10, Sep 2014, Journal of Statistical Software. http://www.jstatsoft.org/v59/i10. "],
["before-we-start.html", "Chapter 3 Before we start 3.1 What is R? What is RStudio? 3.2 Why learn R? 3.3 Knowing your way around RStudio 3.4 Getting set up 3.5 Interacting with R 3.6 How to learn more after the workshop? 3.7 Seeking help", " Chapter 3 Before we start The following material is based on Data Carpentry’s the Data analisis and visualisation lessons. Learning Objectives: Describe the purpose of the RStudio Script, Console, Environment, and Plots panes. Organize files and directories for a set of analyses as an R Project, and understand the purpose of the working directory. Use the built-in RStudio help interface to search for more information on R functions. Demonstrate how to provide sufficient information for troubleshooting with the R user community. 3.1 What is R? What is RStudio? The term “R” is used to refer to both the programming language and the software that interprets the scripts written using it. RStudio is currently a very popular way to not only write your R scripts but also to interact with the R software. To function correctly, RStudio needs R and therefore both need to be installed on your computer. 3.2 Why learn R? 3.2.1 R does not involve lots of pointing and clicking, and that’s a good thing The learning curve might be steeper than with other software, but with R, the results of your analysis does not rely on remembering a succession of pointing and clicking, but instead on a series of written commands, and that’s a good thing! So, if you want to redo your analysis because you collected more data, you don’t have to remember which button you clicked in which order to obtain your results, you just have to run your script again. Working with scripts makes the steps you used in your analysis clear, and the code you write can be inspected by someone else who can give you feedback and spot mistakes. Working with scripts forces you to have a deeper understanding of what you are doing, and facilitates your learning and comprehension of the methods you use. 3.2.2 R code is great for reproducibility Reproducibility is when someone else (including your future self) can obtain the same results from the same dataset when using the same analysis. R integrates with other tools to generate manuscripts from your code. If you collect more data, or fix a mistake in your dataset, the figures and the statistical tests in your manuscript are updated automatically. An increasing number of journals and funding agencies expect analyses to be reproducible, so knowing R will give you an edge with these requirements. 3.2.3 R is interdisciplinary and extensible With 10,000+ packages that can be installed to extend its capabilities, R provides a framework that allows you to combine statistical approaches from many scientific disciplines to best suit the analytical framework you need to analyze your data. For instance, R has packages for image analysis, GIS, time series, population genetics, and a lot more. 3.2.4 R works on data of all shapes and sizes The skills you learn with R scale easily with the size of your dataset. Whether your dataset has hundreds or millions of lines, it won’t make much difference to you. R is designed for data analysis. It comes with special data structures and data types that make handling of missing data and statistical factors convenient. R can connect to spreadsheets, databases, and many other data formats, on your computer or on the web. 3.2.5 R produces high-quality graphics The plotting functionalities in R are endless, and allow you to adjust any aspect of your graph to convey most effectively the message from your data. 3.2.6 R has a large and welcoming community Thousands of people use R daily. Many of them are willing to help you through mailing lists and websites such as Stack Overflow, or on the RStudio community. 3.2.7 Not only is R free, but it is also open-source and cross-platform Anyone can inspect the source code to see how R works. Because of this transparency, there is less chance for mistakes, and if you (or someone else) find some, you can report and fix bugs. 3.3 Knowing your way around RStudio Let’s start by learning about RStudio, which is an Integrated Development Environment (IDE) for working with R. The RStudio IDE open-source product is free under the Affero General Public License (AGPL) v3. The RStudio IDE is also available with a commercial license and priority email support from RStudio, Inc. We will use RStudio IDE to write code, navigate the files on our computer, inspect the variables we are going to create, and visualize the plots we will generate. RStudio can also be used for other things (e.g., version control, developing packages, writing Shiny apps) that we will not cover during the workshop. RStudio interface screenshot RStudio is divided into 4 “Panes”: the Source for your scripts and documents (top-left, in the default layout), the R Console (bottom-left), your Environment/History (top-right), and your Files/Plots/Packages/Help/Viewer (bottom-right). The placement of these panes and their content can be customized (see menu, Tools -&gt; Global Options -&gt; Pane Layout). One of the advantages of using RStudio is that all the information you need to write code is available in a single window. Additionally, with many shortcuts, autocompletion, and highlighting for the major file types you use while developing in R, RStudio will make typing easier and less error-prone. 3.4 Getting set up It is good practice to keep a set of related data, analyses, and text self-contained in a single folder, called the working directory. All of the scripts within this folder can then use relative paths to files that indicate where inside the project a file is located (as opposed to absolute paths, which point to where a file is on a specific computer). Working this way makes it a lot easier to move your project around on your computer and share it with others without worrying about whether or not the underlying scripts will still work. RStudio provides a helpful set of tools to do this through its “Projects” interface, which not only creates a working directory for you but also remembers its location (allowing you to quickly navigate to it) and optionally preserves custom settings and open files to make it easier to resume work after a break. Below, we will go through the steps for creating an “R Project” for this tutorial. Start RStudio (presentation of RStudio -below- should happen here) Under the File menu, click on New project, choose New directory, then New project Enter a name for this new folder (or “directory”), and choose a convenient location for it. This will be your working directory for the rest of the day (e.g., ~/data-carpentry) Click on Create project Under the Files tab on the right of the screen, click on New Folder and create a folder named data within your newly created working directory (e.g., ~/data-carpentry/data) Download the code handout, place it in your working directory and rename it (e.g., data-carpentry-script.R). Your working directory should now look like this: Figure 3.1: How it should look like at the beginning of this lesson 3.4.1 Organizing your working directory Using a consistent folder structure across your projects will help keep things organized, and will also make it easy to find/file things in the future. This can be especially helpful when you have multiple projects. In general, you may create directories (folders) for scripts, data, and documents. data/ Use this folder to store your raw data and intermediate datasets you may create for the need of a particular analysis. For the sake of transparency and provenance, you should always keep a copy of your raw data accessible and do as much of your data cleanup and preprocessing programmatically (i.e., with scripts, rather than manually) as possible. Separating raw data from processed data is also a good idea. For example, you could have files data/raw/tree_survey.plot1.txt and ...plot2.txt kept separate from a data/processed/tree.survey.csv file generated by the scripts/01.preprocess.tree_survey.R script. documents/ This would be a place to keep outlines, drafts, and other text. scripts/ This would be the location to keep your R scripts for different analyses or plotting, and potentially a separate folder for your functions (more on that later). You may want additional directories or subdirectories depending on your project needs, but these should form the backbone of your working directory. For this workshop, we will need a data/ folder to store our raw data, and we will create later a data_output/ folder when we learn how to export data as CSV files. Example of a working directory structure 3.5 Interacting with R The basis of programming is that we write down instructions for the computer to follow, and then we tell the computer to follow those instructions. We write, or code, instructions in R because it is a common language that both the computer and we can understand. We call the instructions commands and we tell the computer to follow the instructions by executing (also called running) those commands. There are two main ways of interacting with R: by using the console or by using script files (plain text files that contain your code). The console pane (in RStudio, the bottom left panel) is the place where commands written in the R language can be typed and executed immediately by the computer. It is also where the results will be shown for commands that have been executed. You can type commands directly into the console and press Enter to execute those commands, but they will be forgotten when you close the session. Because we want our code and workflow to be reproducible, it is better to type the commands we want in the script editor, and save the script. This way, there is a complete record of what we did, and anyone (including our future selves!) can easily replicate the results on their computer. RStudio allows you to execute commands directly from the script editor by using the Ctrl + Enter shortcut (on Macs, Cmd + Return will work, too). The command on the current line in the script (indicated by the cursor) or all of the commands in the currently selected text will be sent to the console and executed when you press Ctrl + Enter. You can find other keyboard shortcuts in this [RStudio cheatsheet about the RStudio IDE](https://github.com/rstudio/cheatsheets/raw/master/rstudio-ide.pdf. At some point in your analysis you may want to check the content of a variable or the structure of an object, without necessarily keeping a record of it in your script. You can type these commands and execute them directly in the console. RStudio provides the Ctrl + 1 and Ctrl + 2 shortcuts allow you to jump between the script and the console panes. If R is ready to accept commands, the R console shows a &gt; prompt. If it receives a command (by typing, copy-pasting or sent from the script editor using Ctrl + Enter), R will try to execute it, and when ready, will show the results and come back with a new &gt; prompt to wait for new commands. If R is still waiting for you to enter more data because it isn’t complete yet, the console will show a + prompt. It means that you haven’t finished entering a complete command. This is because you have not ‘closed’ a parenthesis or quotation, i.e. you don’t have the same number of left-parentheses as right-parentheses, or the same number of opening and closing quotation marks. When this happens, and you thought you finished typing your command, click inside the console window and press Esc; this will cancel the incomplete command and return you to the &gt; prompt. 3.6 How to learn more after the workshop? The material we cover during this workshop will give you an initial taste of how you can use R to analyze data for your own research. However, you will need to learn more to do advanced operations such as cleaning your dataset, using statistical methods, or creating beautiful graphics. The best way to become proficient and efficient at R, as with any other tool, is to use it to address your actual research questions. As a beginner, it can feel daunting to have to write a script from scratch, and given that many people make their code available online, modifying existing code to suit your purpose might make it easier for you to get started. 3.7 Seeking help 3.7.1 Use the built-in RStudio help interface to search for more information on R functions RStudio help interface One of the most fastest ways to get help, is to use the RStudio help interface. This panel by default can be found at the lower right hand panel of RStudio. As seen in the screenshot, by typing the word “Mean”, RStudio tries to also give a number of suggestions that you might be interested in. The description is then shown in the display window. 3.7.2 I know the name of the function I want to use, but I’m not sure how to use it If you need help with a specific function, let’s say barplot(), you can type: ?barplot If you just need to remind yourself of the names of the arguments, you can use: args(lm) 3.7.3 I want to use a function that does X, there must be a function for it but I don’t know which one… If you are looking for a function to do a particular task, you can use the help.search() function, which is called by the double question mark ??. However, this only looks through the installed packages for help pages with a match to your search request ??kruskal If you can’t find what you are looking for, you can use the rdocumentation.org website that searches through the help files across all packages available. Finally, a generic Google or internet search “R &lt;task&gt;” will often either send you to the appropriate package documentation or a helpful forum where someone else has already asked your question. 3.7.4 I am stuck… I get an error message that I don’t understand Start by googling the error message. However, this doesn’t always work very well because often, package developers rely on the error catching provided by R. You end up with general error messages that might not be very helpful to diagnose a problem (e.g. “subscript out of bounds”). If the message is very generic, you might also include the name of the function or package you’re using in your query. However, you should check Stack Overflow. Search using the [r] tag. Most questions have already been answered, but the challenge is to use the right words in the search to find the answers: http://stackoverflow.com/questions/tagged/r The Introduction to R can also be dense for people with little programming experience but it is a good place to understand the underpinnings of the R language. The R FAQ is dense and technical but it is full of useful information. 3.7.5 Asking for help The key to receiving help from someone is for them to rapidly grasp your problem. You should make it as easy as possible to pinpoint where the issue might be. Try to use the correct words to describe your problem. For instance, a package is not the same thing as a library. Most people will understand what you meant, but others have really strong feelings about the difference in meaning. The key point is that it can make things confusing for people trying to help you. Be as precise as possible when describing your problem. If possible, try to reduce what doesn’t work to a simple reproducible example. If you can reproduce the problem using a very small data frame instead of your 50,000 rows and 10,000 columns one, provide the small one with the description of your problem. When appropriate, try to generalize what you are doing so even people who are not in your field can understand the question. For instance instead of using a subset of your real dataset, create a small (3 columns, 5 rows) generic one. For more information on how to write a reproducible example see this article by Hadley Wickham. To share an object with someone else, if it’s relatively small, you can use the function dput(). It will output R code that can be used to recreate the exact same object as the one in memory: dput(head(iris)) # iris is an example data frame that comes with R and head() is a function that returns the first part of the data frame ## structure(list(Sepal.Length = c(5.1, 4.9, 4.7, 4.6, 5, 5.4), ## Sepal.Width = c(3.5, 3, 3.2, 3.1, 3.6, 3.9), Petal.Length = c(1.4, ## 1.4, 1.3, 1.5, 1.4, 1.7), Petal.Width = c(0.2, 0.2, 0.2, ## 0.2, 0.2, 0.4), Species = structure(c(1L, 1L, 1L, 1L, 1L, ## 1L), .Label = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;), class = &quot;factor&quot;)), .Names = c(&quot;Sepal.Length&quot;, ## &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;, &quot;Species&quot;), row.names = c(NA, ## 6L), class = &quot;data.frame&quot;) If the object is larger, provide either the raw file (i.e., your CSV file) with your script up to the point of the error (and after removing everything that is not relevant to your issue). Alternatively, in particular if your question is not related to a data frame, you can save any R object to a file: saveRDS(iris, file=&quot;/tmp/iris.rds&quot;) The content of this file is however not human readable and cannot be posted directly on Stack Overflow. Instead, it can be sent to someone by email who can read it with the readRDS() command (here it is assumed that the downloaded file is in a Downloads folder in the user’s home directory): some_data &lt;- readRDS(file=&quot;~/Downloads/iris.rds&quot;) Last, but certainly not least, always include the output of sessionInfo() as it provides critical information about your platform, the versions of R and the packages that you are using, and other information that can be very helpful to understand your problem. sessionInfo() ## R version 3.4.2 Patched (2017-10-03 r73455) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.5 LTS ## ## Matrix products: default ## BLAS: /usr/lib/atlas-base/libf77blas.so.3.0 ## LAPACK: /usr/lib/lapack/liblapack.so.3.0 ## ## locale: ## [1] LC_CTYPE=en_GB.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_GB.UTF-8 LC_COLLATE=en_GB.UTF-8 ## [5] LC_MONETARY=en_GB.UTF-8 LC_MESSAGES=en_GB.UTF-8 ## [7] LC_PAPER=en_GB.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] methods stats graphics grDevices utils datasets base ## ## other attached packages: ## [1] shiny_1.0.5 ggvis_0.4.3 ggplot2_2.2.1 dplyr_0.7.4 ## [5] BiocStyle_2.5.43 knitr_1.17 DT_0.2 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_0.12.13 msdata_0.17.1 highr_0.6 compiler_3.4.2 ## [5] plyr_1.8.4 bindr_0.1 tools_3.4.2 digest_0.6.12 ## [9] evaluate_0.10.1 tibble_1.3.4 gtable_0.2.0 pkgconfig_2.0.1 ## [13] rlang_0.1.2 rstudioapi_0.7 yaml_2.1.14 bindrcpp_0.2 ## [17] stringr_1.2.0 htmlwidgets_0.9 rprojroot_1.2 grid_3.4.2 ## [21] glue_1.2.0 R6_2.2.2 rmarkdown_1.6 bookdown_0.5 ## [25] magrittr_1.5 backports_1.1.1 scales_0.5.0 htmltools_0.3.6 ## [29] assertthat_0.2.0 xtable_1.8-2 mime_0.5 colorspace_1.3-2 ## [33] httpuv_1.3.5 stringi_1.1.5 lazyeval_0.2.0 munsell_0.4.3 3.7.6 Where to ask for help? The person sitting next to you during the workshop. Don’t hesitate to talk to your neighbor during the workshop, compare your answers, and ask for help. You might also be interested in organizing regular meetings following the workshop to keep learning from each other. Your friendly colleagues: if you know someone with more experience than you, they might be able and willing to help you. Stack Overflow: if your question hasn’t been answered before and is well crafted, chances are you will get an answer in less than 5 min. Remember to follow their guidelines on how to ask a good question. The R-help mailing list: it is read by a lot of people (including most of the R core team), a lot of people post to it, but the tone can be pretty dry, and it is not always very welcoming to new users. If your question is valid, you are likely to get an answer very fast but don’t expect that it will come with smiley faces. Also, here more than anywhere else, be sure to use correct vocabulary (otherwise you might get an answer pointing to the misuse of your words rather than answering your question). You will also have more success if your question is about a base function rather than a specific package. If your question is about a specific package, see if there is a mailing list for it. Usually it’s included in the DESCRIPTION file of the package that can be accessed using packageDescription(&quot;name-of-package&quot;). You may also want to try to email the author of the package directly, or open an issue on the code repository (e.g., GitHub). There are also some topic-specific mailing lists (GIS, phylogenetics, etc…), the complete list is here. 3.7.7 More resources The Posting Guide for the R mailing lists. How to ask for R help useful guidelines This blog post by Jon Skeet has quite comprehensive advice on how to ask programming questions. The reprex package is very helpful to create reproducible examples when asking for help. The [rOpenSci community call “How to ask questions so they get answered”], Github link and video recording includes a presentation of the reprex package and of its philosophy. "],
["introduction-to-r.html", "Chapter 4 Introduction to R 4.1 Creating objects in R 4.2 Vectors and data types 4.3 Subsetting vectors 4.4 Missing data", " Chapter 4 Introduction to R The following material is based on Data Carpentry’s the Data analisis and visualisation lessons. Learning Objectives: Define the following terms as they relate to R: object, assign, call, function, arguments, options. Create objects and and assign values to them. Learn how to name objects Use comments to inform script. Do simple arithmetic operations in R using values and objects. Call functions and use arguments to change their default options. Inspect the content of vectors and manipulate their content. Subset and extract values from vectors. Correctly define and handle missing values in vectors. 4.1 Creating objects in R You can get output from R simply by typing math in the console: 3 + 5 ## [1] 8 12 / 7 ## [1] 1.714286 However, to do useful and interesting things, we need to assign values to objects. To create an object, we need to give it a name followed by the assignment operator &lt;-, and the value we want to give it: weight_kg &lt;- 55 &lt;- is the assignment operator. It assigns values on the right to objects on the left. So, after executing x &lt;- 3, the value of x is 3. The arrow can be read as 3 goes into x. For historical reasons, you can also use = for assignments, but not in every context. Because of the slight differences in syntax, it is good practice to always use &lt;- for assignments. In RStudio, typing Alt + - (push Alt at the same time as the - key) will write &lt;- in a single keystroke. Objects can be given any name such as x, current_temperature, or subject_id. You want your object names to be explicit and not too long. They cannot start with a number (2x is not valid, but x2 is). R is case sensitive (e.g., weight_kg is different from Weight_kg). There are some names that cannot be used because they are the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names (e.g., c, T, mean, data, df, weights). If in doubt, check the help to see if the name is already in use. It’s also best to avoid dots (.) within a variable name as in my.dataset. There are many functions in R with dots in their names for historical reasons, but because dots have a special meaning in R (for methods) and other programming languages, it’s best to avoid them. It is also recommended to use nouns for variable names, and verbs for function names. It’s important to be consistent in the styling of your code (where you put spaces, how you name variables, etc.). Using a consistent coding style makes your code clearer to read for your future self and your collaborators. In R, three popular style guides are Google’s, Jean Fan’s and the tidyverse’s. The tidyverse’s is very comprehensive and may seem overwhelming at first. You can install the lintr to automatically check for issues in the styling of your code. When assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name: weight_kg &lt;- 55 # doesn&#39;t print anything (weight_kg &lt;- 55) # but putting parenthesis around the call prints the value of `weight_kg` ## [1] 55 weight_kg # and so does typing the name of the object ## [1] 55 Now that R has weight_kg in memory, we can do arithmetic with it. For instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg): 2.2 * weight_kg ## [1] 121 We can also change a variable’s value by assigning it a new one: weight_kg &lt;- 57.5 2.2 * weight_kg ## [1] 126.5 This means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a new variable, weight_lb: weight_lb &lt;- 2.2 * weight_kg and then change weight_kg to 100. weight_kg &lt;- 100 What do you think is the current content of the object weight_lb? 126.5 or 220? 4.1.1 Comments The comment character in R is #, anything to the right of a # in a script will be ignored by R. It is useful to leave notes, and explanations in your scripts. RStudio makes it easy to comment or uncomment a paragraph: after selecting the lines you want to comment, press at the same time on your keyboard Ctrl + Shift + C. If you only want to comment out one line, you can put the cursor at any location of that line (i.e. no need to select the whole line), then press Ctrl + Shift + C. Challenge What are the values after each statement in the following? mass &lt;- 47.5 # mass? age &lt;- 122 # age? mass &lt;- mass * 2.0 # mass? age &lt;- age - 20 # age? mass_index &lt;- mass/age # mass_index? 4.1.2 Functions and their arguments Functions are “canned scripts” that automate more complicated sets of commands including operations assignments, etc. Many functions are predefined, or can be made available by importing R packages (more on that later). A function usually gets one or more inputs called arguments. Functions often (but not always) return a value. A typical example would be the function sqrt(). The input (the argument) must be a number, and the return value (in fact, the output) is the square root of that number. Executing a function (‘running it’) is called calling the function. An example of a function call is: b &lt;- sqrt(a) Here, the value of a is given to the sqrt() function, the sqrt() function calculates the square root, and returns the value which is then assigned to variable b. This function is very simple, because it takes just one argument. The return ‘value’ of a function need not be numerical (like that of sqrt()), and it also does not need to be a single item: it can be a set of things, or even a dataset. We’ll see that when we read data files into R. Arguments can be anything, not only numbers or filenames, but also other objects. Exactly what each argument means differs per function, and must be looked up in the documentation (see below). Some functions take arguments which may either be specified by the user, or, if left out, take on a default value: these are called options. Options are typically used to alter the way the function operates, such as whether it ignores ‘bad values’, or what symbol to use in a plot. However, if you want something specific, you can specify a value of your choice which will be used instead of the default. Let’s try a function that can take multiple arguments: round(). round(3.14159) ## [1] 3 Here, we’ve called round() with just one argument, 3.14159, and it has returned the value 3. That’s because the default is to round to the nearest whole number. If we want more digits we can see how to do that by getting information about the round function. We can use args(round) or look at the help for this function using ?round. args(round) ## function (x, digits = 0) ## NULL ?round We see that if we want a different number of digits, we can type digits=2 or however many we want. round(3.14159, digits = 2) ## [1] 3.14 If you provide the arguments in the exact same order as they are defined you don’t have to name them: round(3.14159, 2) ## [1] 3.14 And if you do name the arguments, you can switch their order: round(digits = 2, x = 3.14159) ## [1] 3.14 It’s good practice to put the non-optional arguments (like the number you’re rounding) first in your function call, and to specify the names of all optional arguments. If you don’t, someone reading your code might have to look up the definition of a function with unfamiliar arguments to understand what you’re doing. 4.1.3 Objects vs. variables What are known as objects in R are known as variables in many other programming languages. Depending on the context, object and variable can have drastically different meanings. However, in this lesson, the two words are used synonymously. For more information see: https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Objects 4.2 Vectors and data types A vector is the most common and basic data type in R, and is pretty much the workhorse of R. A vector is composed by a series of values, which can be either numbers or characters. We can assign a series of values to a vector using the c() function. For example we can create a vector of animal weights and assign it to a new object weight_g: weight_g &lt;- c(50, 60, 65, 82) weight_g ## [1] 50 60 65 82 A vector can also contain characters: animals &lt;- c(&quot;mouse&quot;, &quot;rat&quot;, &quot;dog&quot;) animals ## [1] &quot;mouse&quot; &quot;rat&quot; &quot;dog&quot; The quotes around “mouse”, “rat”, etc. are essential here. Without the quotes R will assume there are objects called mouse, rat and dog. As these objects don’t exist in R’s memory, there will be an error message. There are many functions that allow you to inspect the content of a vector. length() tells you how many elements are in a particular vector: length(weight_g) ## [1] 4 length(animals) ## [1] 3 An important feature of a vector, is that all of the elements are the same type of data. The function class() indicates the class (the type of element) of an object: class(weight_g) ## [1] &quot;numeric&quot; class(animals) ## [1] &quot;character&quot; The function str() provides an overview of the structure of an object and its elements. It is a useful function when working with large and complex objects: str(weight_g) ## num [1:4] 50 60 65 82 str(animals) ## chr [1:3] &quot;mouse&quot; &quot;rat&quot; &quot;dog&quot; You can use the c() function to add other elements to your vector: weight_g &lt;- c(weight_g, 90) # add to the end of the vector weight_g &lt;- c(30, weight_g) # add to the beginning of the vector weight_g ## [1] 30 50 60 65 82 90 In the first line, we take the original vector weight_g, add the value 90 to the end of it, and save the result back into weight_g. Then we add the value 30 to the beginning, again saving the result back into weight_g. We can do this over and over again to grow a vector, or assemble a dataset. As we program, this may be useful to add results that we are collecting or calculating. An atomic vector is the simplest R data type and is a linear vector of a single type. Above, we saw 2 of the 6 main atomic vector types that R uses: &quot;character&quot; and &quot;numeric&quot; (or &quot;double&quot;). These are the basic building blocks that all R objects are built from. The other 4 atomic vector types are: &quot;logical&quot; for TRUE and FALSE (the boolean data type) &quot;integer&quot; for integer numbers (e.g., 2L, the L indicates to R that it’s an integer) &quot;complex&quot; to represent complex numbers with real and imaginary parts (e.g., 1 + 4i) and that’s all we’re going to say about them &quot;raw&quot; for bitstreams that we won’t discuss further You can check the type of your vector using the typeof() function and inputting your vector as the argument. Vectors are one of the many data structures that R uses. Other important ones are lists (list), matrices (matrix), data frames (data.frame), factors (factor) and arrays (array). Challenge We’ve seen that atomic vectors can be of type character, numeric (or double), integer, and logical. But what happens if we try to mix these types in a single vector? What will happen in each of these examples? (hint: use class() to check the data type of your objects): num_char &lt;- c(1, 2, 3, &#39;a&#39;) num_logical &lt;- c(1, 2, 3, TRUE) char_logical &lt;- c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, TRUE) tricky &lt;- c(1, 2, 3, &#39;4&#39;) Why do you think it happens? You’ve probably noticed that objects of different types get converted into a single, shared type within a vector. In R, we call converting objects from one class into another class coercion. These conversions happen according to a hierarchy, whereby some types get preferentially coerced into other types. Can you draw a diagram that represents the hierarchy of how these data types are coerced? logical -&gt; numeric -&gt; character &lt;- logical 4.3 Subsetting vectors If we want to extract one or several values from a vector, we must provide one or several indices in square brackets. For instance: animals &lt;- c(&quot;mouse&quot;, &quot;rat&quot;, &quot;dog&quot;, &quot;cat&quot;) animals[2] ## [1] &quot;rat&quot; animals[c(3, 2)] ## [1] &quot;dog&quot; &quot;rat&quot; We can also repeat the indices to create an object with more elements than the original one: more_animals &lt;- animals[c(1, 2, 3, 2, 1, 4)] more_animals ## [1] &quot;mouse&quot; &quot;rat&quot; &quot;dog&quot; &quot;rat&quot; &quot;mouse&quot; &quot;cat&quot; R indices start at 1. Programming languages like Fortran, MATLAB, Julia, and R start counting at 1, because that’s what human beings typically do. Languages in the C family (including C++, Java, Perl, and Python) count from 0 because that’s simpler for computers to do. 4.3.1 Conditional subsetting Another common way of subsetting is by using a logical vector. TRUE will select the element with the same index, while FALSE will not: weight_g &lt;- c(21, 34, 39, 54, 55) weight_g[c(TRUE, FALSE, TRUE, TRUE, FALSE)] ## [1] 21 39 54 Typically, these logical vectors are not typed by hand, but are the output of other functions or logical tests. For instance, if you wanted to select only the values above 50: weight_g &gt; 50 # will return logicals with TRUE for the indices that meet the condition ## [1] FALSE FALSE FALSE TRUE TRUE ## so we can use this to select only the values above 50 weight_g[weight_g &gt; 50] ## [1] 54 55 You can combine multiple tests using &amp; (both conditions are true, AND) or | (at least one of the conditions is true, OR): weight_g[weight_g &lt; 30 | weight_g &gt; 50] ## [1] 21 54 55 weight_g[weight_g &gt;= 30 &amp; weight_g == 21] ## numeric(0) Here, &lt; stands for “less than”, &gt; for “greater than”, &gt;= for “greater than or equal to”, and == for “equal to”. The double equal sign == is a test for numerical equality between the left and right hand sides, and should not be confused with the single = sign, which performs variable assignment (similar to &lt;-). A common task is to search for certain strings in a vector. One could use the “or” operator | to test for equality to multiple values, but this can quickly become tedious. The function %in% allows you to test if any of the elements of a search vector are found: animals &lt;- c(&quot;mouse&quot;, &quot;rat&quot;, &quot;dog&quot;, &quot;cat&quot;) animals[animals == &quot;cat&quot; | animals == &quot;rat&quot;] # returns both rat and cat ## [1] &quot;rat&quot; &quot;cat&quot; animals %in% c(&quot;rat&quot;, &quot;cat&quot;, &quot;dog&quot;, &quot;duck&quot;, &quot;goat&quot;) ## [1] FALSE TRUE TRUE TRUE animals[animals %in% c(&quot;rat&quot;, &quot;cat&quot;, &quot;dog&quot;, &quot;duck&quot;, &quot;goat&quot;)] ## [1] &quot;rat&quot; &quot;dog&quot; &quot;cat&quot; Challenge (optional) Can you figure out why &quot;four&quot; &gt; &quot;five&quot; returns TRUE? When using “&gt;” or “&lt;” on strings, R compares their alphabetical order. Here “four” comes after “five”, and therefore is “greater than” it. 4.4 Missing data As R was designed to analyze datasets, it includes the concept of missing data (which is uncommon in other programming languages). Missing data are represented in vectors as NA. When doing operations on numbers, most functions will return NA if the data you are working with include missing values. This feature makes it harder to overlook the cases where you are dealing with missing data. You can add the argument na.rm=TRUE to calculate the result while ignoring the missing values. heights &lt;- c(2, 4, 4, NA, 6) mean(heights) ## [1] NA max(heights) ## [1] NA mean(heights, na.rm = TRUE) ## [1] 4 max(heights, na.rm = TRUE) ## [1] 6 If your data include missing values, you may want to become familiar with the functions is.na(), na.omit(), and complete.cases(). See below for examples. ## Extract those elements which are not missing values. heights[!is.na(heights)] ## [1] 2 4 4 6 ## Returns the object with incomplete cases removed. The returned object is an atomic vector of type `&quot;numeric&quot;` (or `&quot;double&quot;`). na.omit(heights) ## [1] 2 4 4 6 ## attr(,&quot;na.action&quot;) ## [1] 4 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; ## Extract those elements which are complete cases. The returned object is an atomic vector of type `&quot;numeric&quot;` (or `&quot;double&quot;`). heights[complete.cases(heights)] ## [1] 2 4 4 6 Recall that you can use the typeof() function to find the type of your atomic vector. 4.4 Challenge Using this vector of length measurements, create a new vector with the NAs removed. lengths &lt;- c(10,24,NA,18,NA,20) Use the function median() to calculate the median of the lengths vector. Now that we have learned how to write scripts, and the basics of R’s data structures, we are ready to start working with the Portal dataset we have been using in the other lessons, and learn about data frames. "],
["starting-with-data.html", "Chapter 5 Starting with data 5.1 Presentation of the Survey Data 5.2 What are data frames? 5.3 Inspecting data.frame Objects 5.4 Indexing and subsetting data frames 5.5 Factors 5.6 Formatting Dates", " Chapter 5 Starting with data The following material is based on Data Carpentry’s the Data analisis and visualisation lessons. Learning Objectives: Describe what a data frame is. Load external data from a .csv file into a data frame in R. Summarize the contents of a data frame in R. Manipulate categorical data in R. Change how character strings are handled in a data frame. Format dates in R 5.1 Presentation of the Survey Data We are studying the species and weight of animals caught in plots in our study area. The dataset is stored as a comma separated value (CSV) file. Each row holds information for a single animal, and the columns represent: Column Description record_id Unique id for the observation month month of observation day day of observation year year of observation plot_id ID of a particular plot species_id 2-letter code sex sex of animal (“M”, “F”) hindfoot_length length of the hindfoot in mm weight weight of the animal in grams genus genus of animal species species of animal taxa e.g. Rodent, Reptile, Bird, Rabbit plot_type type of plot We are going to use the R function download.file() to download the CSV file that contains the survey data from figshare, and we will use read.csv() to load into memory the content of the CSV file as an object of class data.frame. To download the data into the data/ subdirectory, run the following: download.file(&quot;https://ndownloader.figshare.com/files/2292169&quot;, &quot;data/portal_data_joined.csv&quot;) You are now ready to load the data: surveys &lt;- read.csv(&#39;data/portal_data_joined.csv&#39;) This statement doesn’t produce any output because, as you might recall, assignments don’t display anything. If we want to check that our data has been loaded, we can print the variable’s value: surveys. Wow… that was a lot of output. At least it means the data loaded properly. Let’s check the top (the first 6 lines) of this data frame using the function head(): head(surveys) ## record_id month day year plot_id species_id sex hindfoot_length weight ## 1 1 7 16 1977 2 NL M 32 NA ## 2 72 8 19 1977 2 NL M 31 NA ## 3 224 9 13 1977 2 NL NA NA ## 4 266 10 16 1977 2 NL NA NA ## 5 349 11 12 1977 2 NL NA NA ## 6 363 11 12 1977 2 NL NA NA ## genus species taxa plot_type ## 1 Neotoma albigula Rodent Control ## 2 Neotoma albigula Rodent Control ## 3 Neotoma albigula Rodent Control ## 4 Neotoma albigula Rodent Control ## 5 Neotoma albigula Rodent Control ## 6 Neotoma albigula Rodent Control Note read.csv assumes that fields are delineated by commas, however, in several countries, the comma is used as a decimal separator and the semicolon (;) is used as a field delineator. If you want to read in this type of files in R, you can use the read.csv2 function. It behaves exactly like read.csv but uses different parameters for the decimal and the field separators. If you are working with other format, they can be both specified by the user. Check out the help for read.csv() to learn more. 5.2 What are data frames? Data frames are the de facto data structure for most tabular data, and what we use for statistics and plotting. A data frame can be created by hand, but most commonly they are generated by the functions read.csv() or read.table(); in other words, when importing spreadsheets from your hard drive (or the web). A data frame is the representation of data in the format of a table where the columns are vectors that all have the same length. Because the column are vectors, they all contain the same type of data (e.g., characters, integers, factors). We can see this when inspecting the structure of a data frame with the function str(): str(surveys) ## &#39;data.frame&#39;: 34786 obs. of 13 variables: ## $ record_id : int 1 72 224 266 349 363 435 506 588 661 ... ## $ month : int 7 8 9 10 11 11 12 1 2 3 ... ## $ day : int 16 19 13 16 12 12 10 8 18 11 ... ## $ year : int 1977 1977 1977 1977 1977 1977 1977 1978 1978 1978 ... ## $ plot_id : int 2 2 2 2 2 2 2 2 2 2 ... ## $ species_id : Factor w/ 48 levels &quot;AB&quot;,&quot;AH&quot;,&quot;AS&quot;,..: 16 16 16 16 16 16 16 16 16 16 ... ## $ sex : Factor w/ 3 levels &quot;&quot;,&quot;F&quot;,&quot;M&quot;: 3 3 1 1 1 1 1 1 3 1 ... ## $ hindfoot_length: int 32 31 NA NA NA NA NA NA NA NA ... ## $ weight : int NA NA NA NA NA NA NA NA 218 NA ... ## $ genus : Factor w/ 26 levels &quot;Ammodramus&quot;,&quot;Ammospermophilus&quot;,..: 13 13 13 13 13 13 13 13 13 13 ... ## $ species : Factor w/ 40 levels &quot;albigula&quot;,&quot;audubonii&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ taxa : Factor w/ 4 levels &quot;Bird&quot;,&quot;Rabbit&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... ## $ plot_type : Factor w/ 5 levels &quot;Control&quot;,&quot;Long-term Krat Exclosure&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 5.3 Inspecting data.frame Objects We already saw how the functions head() and str() can be useful to check the content and the structure of a data frame. Here is a non-exhaustive list of functions to get a sense of the content/structure of the data. Let’s try them out! Size: dim(surveys) - returns a vector with the number of rows in the first element, and the number of columns as the second element (the dimensions of the object) nrow(surveys) - returns the number of rows ncol(surveys) - returns the number of columns Content: head(surveys) - shows the first 6 rows tail(surveys) - shows the last 6 rows Names: names(surveys) - returns the column names (synonym of colnames() for data.frame objects) rownames(surveys) - returns the row names Summary: str(surveys) - structure of the object and information about the class, length and content of each column summary(surveys) - summary statistics for each column Note: most of these functions are “generic”, they can be used on other types of objects besides data.frame. Challenge Based on the output of str(surveys), can you answer the following questions? What is the class of the object surveys? How many rows and how many columns are in this object? How many species have been recorded during these surveys? 5.4 Indexing and subsetting data frames Our survey data frame has rows and columns (it has 2 dimensions), if we want to extract some specific data from it, we need to specify the “coordinates” we want from it. Row numbers come first, followed by column numbers. However, note that different ways of specifying these coordinates lead to results with different classes. surveys[1, 1] # first element in the first column of the data frame (as a vector) surveys[1, 6] # first element in the 6th column (as a vector) surveys[, 1] # first column in the data frame (as a vector) surveys[1] # first column in the data frame (as a data.frame) surveys[1:3, 7] # first three elements in the 7th column (as a vector) surveys[3, ] # the 3rd element for all columns (as a data.frame) head_surveys &lt;- surveys[1:6, ] # equivalent to head_surveys &lt;- head(surveys) : is a special function that creates numeric vectors of integers in increasing or decreasing order, test 1:10 and 10:1 for instance. You can also exclude certain parts of a data frame using the “-” sign: surveys[,-1] # The whole data frame, except the first column surveys[-c(7:34786),] # Equivalent to head(surveys) As well as using numeric values to subset a data.frame (or matrix), columns can be called by name, using one of the four following notations: surveys[&quot;species_id&quot;] # Result is a data.frame surveys[, &quot;species_id&quot;] # Result is a vector surveys[[&quot;species_id&quot;]] # Result is a vector surveys$species_id # Result is a vector For our purposes, the last three notations are equivalent. RStudio knows about the columns in your data frame, so you can take advantage of the autocompletion feature to get the full and correct column name. Challenge Create a data.frame (surveys_200) containing only the observations from row 200 of the surveys dataset. Notice how nrow() gave you the number of rows in a data.frame? Use that number to pull out just that last row in the data frame. Compare that with what you see as the last row using tail() to make sure it’s meeting expectations. Pull out that last row using nrow() instead of the row number. Create a new data frame object (surveys_last) from that last row. Use nrow() to extract the row that is in the middle of the data frame. Store the content of this row in an object named surveys_middle. Combine nrow() with the - notation above to reproduce the behavior of head(surveys) keeping just the first through 6th rows of the surveys dataset. ## Answers surveys_200 &lt;- surveys[200, ] surveys_last &lt;- surveys[nrow(surveys), ] surveys_middle &lt;- surveys[nrow(surveys)/2, ] surveys_head &lt;- surveys[-c(7:nrow(surveys)),] 5.5 Factors When we did str(surveys) we saw that several of the columns consist of integers, however, the columns genus, species, sex, plot_type, … are of a special class called a factor. Factors are very useful and are actually something that make R particularly well suited to working with data, so we’re going to spend a little time introducing them. Factors are used to represent categorical data. Factors can be ordered or unordered, and understanding them is necessary for statistical analysis and for plotting. Factors are stored as integers, and have labels (text) associated with these unique integers. While factors look (and often behave) like character vectors, they are actually integers under the hood, and you need to be careful when treating them like strings. Once created, factors can only contain a pre-defined set of values, known as levels. By default, R always sorts levels in alphabetical order. For instance, if you have a factor with 2 levels: sex &lt;- factor(c(&quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;)) R will assign 1 to the level &quot;female&quot; and 2 to the level &quot;male&quot; (because f comes before m, even though the first element in this vector is &quot;male&quot;). You can check this by using the function levels(), and check the number of levels using nlevels(): levels(sex) ## [1] &quot;female&quot; &quot;male&quot; nlevels(sex) ## [1] 2 Sometimes, the order of the factors does not matter, other times you might want to specify the order because it is meaningful (e.g., “low”, “medium”, “high”), it improves your visualization, or it is required by a particular type of analysis. Here, one way to reorder our levels in the sex vector would be: sex # current order ## [1] male female female male ## Levels: female male sex &lt;- factor(sex, levels = c(&quot;male&quot;, &quot;female&quot;)) sex # after re-ordering ## [1] male female female male ## Levels: male female In R’s memory, these factors are represented by integers (1, 2, 3), but are more informative than integers because factors are self describing: &quot;female&quot;, &quot;male&quot; is more descriptive than 1, 2. Which one is “male”? You wouldn’t be able to tell just from the integer data. Factors, on the other hand, have this information built in. It is particularly helpful when there are many levels (like the species names in our example dataset). 5.5.0.1 Converting factors If you need to convert a factor to a character vector, you use as.character(x). as.character(sex) ## [1] &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;male&quot; Converting factors where the levels appear as numbers (such as concentration levels, or years) to a numeric vector is a little trickier. The as.numeric() function returns the index values of the factor, not its levels, so it will result in an entirely new (and unwanted in this case) set of numbers. One method to avoid this is to convert factors to characters and then numbers. Another method is to use the levels() function. Compare: f &lt;- factor(c(1990, 1983, 1977, 1998, 1990)) as.numeric(f) # Wrong! And there is no warning... ## [1] 3 2 1 4 3 as.numeric(as.character(f)) # Works... ## [1] 1990 1983 1977 1998 1990 as.numeric(levels(f))[f] # The recommended way. ## [1] 1990 1983 1977 1998 1990 Notice that in the levels() approach, three important steps occur: We obtain all the factor levels using levels(f) We convert these levels to numeric values using as.numeric(levels(f)) We then access these numeric values using the underlying integers of the vector f inside the square brackets 5.5.0.2 Renaming factors When your data is stored as a factor, you can use the plot() function to get a quick glance at the number of observations represented by each factor level. Let’s look at the number of males and females captured over the course of the experiment: ## bar plot of the number of females and males captured during the experiment: plot(surveys$sex) In addition to males and females, there are about 1700 individuals for which the sex information hasn’t been recorded. Additionally, for these individuals, there is no label to indicate that the information is missing. Let’s rename this label to something more meaningful. Before doing that, we’re going to pull out the data on sex and work with that data, so we’re not modifying the working copy of the data frame: sex &lt;- surveys$sex head(sex) ## [1] M M ## Levels: F M levels(sex) ## [1] &quot;&quot; &quot;F&quot; &quot;M&quot; levels(sex)[1] &lt;- &quot;missing&quot; levels(sex) ## [1] &quot;missing&quot; &quot;F&quot; &quot;M&quot; head(sex) ## [1] M M missing missing missing missing ## Levels: missing F M Challenge Rename “F” and “M” to “female” and “male” respectively. Now that we have renamed the factor level to “missing”, can you recreate the barplot such that “missing” is last (after “male”)? ## Answers levels(sex)[2:3] &lt;- c(&quot;female&quot;, &quot;male&quot;) sex &lt;- factor(sex, levels = c(&quot;female&quot;, &quot;male&quot;, &quot;missing&quot;)) plot(sex) 5.5.0.3 Using stringsAsFactors=FALSE By default, when building or importing a data frame, the columns that contain characters (i.e., text) are coerced (=converted) into the factor data type. Depending on what you want to do with the data, you may want to keep these columns as character. To do so, read.csv() and read.table() have an argument called stringsAsFactors which can be set to FALSE. In most cases, it’s preferable to set stringsAsFactors = FALSE when importing your data, and converting as a factor only the columns that require this data type. Compare the output of str(surveys) when setting stringsAsFactors = TRUE (default) and stringsAsFactors = FALSE: ## Compare the difference between when the data are being read as ## `factor`, and when they are being read as `character`. surveys &lt;- read.csv(&quot;data/portal_data_joined.csv&quot;, stringsAsFactors = TRUE) str(surveys) surveys &lt;- read.csv(&quot;data/portal_data_joined.csv&quot;, stringsAsFactors = FALSE) str(surveys) ## Convert the column &quot;plot_type&quot; into a factor surveys$plot_type &lt;- factor(surveys$plot_type) Challenge We have seen how data frames are created when using the read.csv(), but they can also be created by hand with the data.frame() function. There are a few mistakes in this hand-crafted data.frame, can you spot and fix them? Don’t hesitate to experiment! animal_data &lt;- data.frame(animal=c(&quot;dog&quot;, &quot;cat&quot;, &quot;sea cucumber&quot;, &quot;sea urchin&quot;), feel=c(&quot;furry&quot;, &quot;squishy&quot;, &quot;spiny&quot;), weight=c(45, 8 1.1, 0.8)) Can you predict the class for each of the columns in the following example? Check your guesses using str(country_climate): Are they what you expected? Why? Why not? What would have been different if we had added stringsAsFactors = FALSE to this call? What would you need to change to ensure that each column had the accurate data type? country_climate &lt;- data.frame( country=c(&quot;Canada&quot;, &quot;Panama&quot;, &quot;South Africa&quot;, &quot;Australia&quot;), climate=c(&quot;cold&quot;, &quot;hot&quot;, &quot;temperate&quot;, &quot;hot/temperate&quot;), temperature=c(10, 30, 18, &quot;15&quot;), northern_hemisphere=c(TRUE, TRUE, FALSE, &quot;FALSE&quot;), has_kangaroo=c(FALSE, FALSE, FALSE, 1) ) * missing quotations around the names of the animals * missing one entry in the “feel” column (probably for one of the furry animals) * missing one comma in the weight column * country, climate, temperature, and northern_hemisphere are factors; has_kangaroo is numeric. * using stringsAsFactors=FALSE would have made them character instead of factors * removing the quotes in temperature, northern_hemisphere, and replacing 1 by TRUE in the has_kangaroo column would probably what was originally intended. The automatic conversion of data type is sometimes a blessing, sometimes an annoyance. Be aware that it exists, learn the rules, and double check that data you import in R are of the correct type within your data frame. If not, use it to your advantage to detect mistakes that might have been introduced during data entry (a letter in a column that should only contain numbers for instance). 5.6 Formatting Dates One of the most common issues that new (and experienced!) R users have is converting date and time information into a variable that is appropriate and usable during analyses. As a reminder from earlier in this lesson, the best practice for dealing with date data is to ensure that each component of your date is stored as a separate variable. Using str(), We can confirm that our data frame has a separate column for day, month, and year, and each contains integer values. str(surveys) We’re going to be using the ymd() function from the package lubridate (this package gets installed during the installation of the tidyverse package). This function is designed to take a vector representing year, month, and day and convert that information to a POSIXct vector. POSIXct is a class of data recognized by R as being a date or date and time. The argument that the function requires is relatively flexible, but, as a best practice, is a character vector formatted as “YYYY-MM-DD”. Start by loading the required package: library(lubridate) Create a character vector from the year, month, and day columns of surveys using paste(): paste(surveys$year, surveys$month, surveys$day, sep=&quot;-&quot;) # sep indicates the character to use to separate each component This character vector can be used as the argument for ymd(): ymd(paste(surveys$year, surveys$month, surveys$day, sep=&quot;-&quot;)) The resulting POSIXct vector can be added to surveys as a new column called date: surveys$date &lt;- ymd(paste(surveys$year, surveys$month, surveys$day, sep=&quot;-&quot;)) ## Warning: 129 failed to parse. str(surveys) # notice the new column, with &#39;date&#39; as the class ## &#39;data.frame&#39;: 34786 obs. of 14 variables: ## $ record_id : int 1 72 224 266 349 363 435 506 588 661 ... ## $ month : int 7 8 9 10 11 11 12 1 2 3 ... ## $ day : int 16 19 13 16 12 12 10 8 18 11 ... ## $ year : int 1977 1977 1977 1977 1977 1977 1977 1978 1978 1978 ... ## $ plot_id : int 2 2 2 2 2 2 2 2 2 2 ... ## $ species_id : Factor w/ 48 levels &quot;AB&quot;,&quot;AH&quot;,&quot;AS&quot;,..: 16 16 16 16 16 16 16 16 16 16 ... ## $ sex : Factor w/ 3 levels &quot;&quot;,&quot;F&quot;,&quot;M&quot;: 3 3 1 1 1 1 1 1 3 1 ... ## $ hindfoot_length: int 32 31 NA NA NA NA NA NA NA NA ... ## $ weight : int NA NA NA NA NA NA NA NA 218 NA ... ## $ genus : Factor w/ 26 levels &quot;Ammodramus&quot;,&quot;Ammospermophilus&quot;,..: 13 13 13 13 13 13 13 13 13 13 ... ## $ species : Factor w/ 40 levels &quot;albigula&quot;,&quot;audubonii&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ taxa : Factor w/ 4 levels &quot;Bird&quot;,&quot;Rabbit&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... ## $ plot_type : Factor w/ 5 levels &quot;Control&quot;,&quot;Long-term Krat Exclosure&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ date : Date, format: &quot;1977-07-16&quot; &quot;1977-08-19&quot; ... Did you notice the warning message? Let’s investigate what is happening. When lubridate fails to parse a date, it will return NA. We can use the functions we saw previously to deal with missing data to identify the rows in our data frame that are failing. First, let’s create a vector that contains our dates as a character vector: surveys_dates &lt;- paste(surveys$year, surveys$month, surveys$day, sep = &quot;-&quot;) head(surveys_dates) ## [1] &quot;1977-7-16&quot; &quot;1977-8-19&quot; &quot;1977-9-13&quot; &quot;1977-10-16&quot; &quot;1977-11-12&quot; ## [6] &quot;1977-11-12&quot; The vector surveys_dates contains all the dates from our dataset, in the same order as they are in the data frame. We can therefore use the is.na() function on the surveys data frame to locate the elements in the vectors surveys_dates that failed to parse: surveys_dates[is.na(surveys$date)] ## [1] &quot;2000-9-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; ## [6] &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; ## [11] &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-9-31&quot; ## [16] &quot;2000-9-31&quot; &quot;2000-4-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-4-31&quot; ## [21] &quot;2000-4-31&quot; &quot;2000-9-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; ## [26] &quot;2000-4-31&quot; &quot;2000-9-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-9-31&quot; ## [31] &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-4-31&quot; ## [36] &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; ## [41] &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-9-31&quot; ## [46] &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; ## [51] &quot;2000-4-31&quot; &quot;2000-9-31&quot; &quot;2000-4-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; ## [56] &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-9-31&quot; ## [61] &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-4-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; ## [66] &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; ## [71] &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-4-31&quot; ## [76] &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; ## [81] &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; ## [86] &quot;2000-9-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; ## [91] &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-9-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; ## [96] &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-4-31&quot; &quot;2000-9-31&quot; ## [ reached getOption(&quot;max.print&quot;) -- omitted 29 entries ] Why did these dates fail to parse? If you had to use these data for your analyses, how would you deal with this situation? "],
["manipulating-and-analyzing-data.html", "Chapter 6 Manipulating and analyzing data 6.1 What are dplyr and tidyr? 6.2 Selecting columns and filtering rows 6.3 Pipes 6.4 Exporting data", " Chapter 6 Manipulating and analyzing data The following material is based on Data Carpentry’s the Data analisis and visualisation lessons. Learning Objectives: Understand the purpose of the dplyr and tidyr packages. Select certain columns in a data frame with the dplyr function select. Select certain rows in a data frame according to filtering conditions with the dplyr function filter . Link the output of one dplyr function to the input of another function with the ‘pipe’ operator. Add new columns to a data frame that are functions of existing columns with mutate. Understand the split-apply-combine concept for data analysis. Use summarize, group_by, and tally to split a data frame into groups of observations, apply a summary statistics for each group, and then combine the results. Understand the concept of a wide and a long table format and for which purpose those formats are useful. Understand what key-value pairs are. Reshape a data frame from long to wide format and back with the spread and gather commands from the tidyr package. Export a data frame to a .csv file. Bracket subsetting is handy, but it can be cumbersome and difficult to read, especially for complicated operations. Enter dplyr. dplyr is a package for making tabular data manipulation easier. It pairs nicely with tidyr which enables you to swiftly convert between different data formats for plotting and analysis. Packages in R are basically sets of additional functions that let you do more stuff. The functions we’ve been using so far, like str() or data.frame(), come built into R; packages give you access to more of them. Before you use a package for the first time you need to install it on your machine, and then you should import it in every subsequent R session when you need it. You should already have installed the tidyverse package. This is an “umbrella-package” that installs several packages useful for data analysis which work together well such as tidyr, dplyr, ggplot2, tibble, etc. tidyverse package tries to address 3 major problems with some of base R functions: 1. The results from a base R function sometimes depends on the type of data. 2. Using R expressions in a non standard way, which can be confusing for new learners. 3. Hidden arguments, having default operations that new learners are not aware of. We have seen in our previous lesson that when building or importing a data frame, the columns that contain characters (i.e., text) are coerced (=converted) into the factor data type. We had to set stringsAsFactor to FALSE to avoid this hidden argument to convert our data type. This time will use the tidyverse package to read the data and avoid having to set stringsAsFactor to FALSE To load the package type: library(&quot;tidyverse&quot;) ## load the tidyverse packages, incl. dplyr 6.1 What are dplyr and tidyr? The package dplyr provides easy tools for the most common data manipulation tasks. It is built to work directly with data frames, with many common tasks optimized by being written in a compiled language (C++). An additional feature is the ability to work directly with data stored in an external database. The benefits of doing this are that the data can be managed natively in a relational database, queries can be conducted on that database, and only the results of the query are returned. This addresses a common problem with R in that all operations are conducted in-memory and thus the amount of data you can work with is limited by available memory. The database connections essentially remove that limitation in that you can have a database of many 100s GB, conduct queries on it directly, and pull back into R only what you need for analysis. The package tidyr addresses the common problem of wanting to reshape your data for plotting and use by different R functions. Sometimes we want data sets where we have one row per measurement. Sometimes we want a data frame where each measurement type has its own column, and rows are instead more aggregated groups - like plots or aquaria. Moving back and forth between these formats is nontrivial, and tidyr gives you tools for this and more sophisticated data manipulation. To learn more about dplyr and tidyr after the workshop, you may want to check out this handy data transformation with dplyr cheatsheet and this one about tidyr. dplyr reads data using read_csv(), instead of read.csv() surveys &lt;- read_csv(&#39;data/portal_data_joined.csv&#39;) ## Parsed with column specification: ## cols( ## record_id = col_integer(), ## month = col_integer(), ## day = col_integer(), ## year = col_integer(), ## plot_id = col_integer(), ## species_id = col_character(), ## sex = col_character(), ## hindfoot_length = col_integer(), ## weight = col_integer(), ## genus = col_character(), ## species = col_character(), ## taxa = col_character(), ## plot_type = col_character() ## ) ## inspect the data str(surveys) Notice that the class of the data is now tbl_df This is referred to as a “tibble” Tibbles are data frames, but they tweak some of the old behaviors of data frames. The data structure is very similar to a data frame. For our purposes the only differences are that: In addition to displaying the data type of each column under its name, it only prints the first few rows of data and only as many columns as fit on one screen. Columns of class character are never converted into factors. 6.2 Selecting columns and filtering rows We’re going to learn some of the most common dplyr functions: select(), filter(), mutate(), group_by(), and summarize(). To select columns of a data frame, use select(). The first argument to this function is the data frame (surveys), and the subsequent arguments are the columns to keep. select(surveys, plot_id, species_id, weight) To choose rows based on a specific criteria, use filter(): filter(surveys, year == 1995) ## # A tibble: 1,180 x 13 ## record_id month day year plot_id species_id sex hindfoot_length ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 22314 6 7 1995 2 NL M 34 ## 2 22728 9 23 1995 2 NL F 32 ## 3 22899 10 28 1995 2 NL F 32 ## 4 23032 12 2 1995 2 NL F 33 ## 5 22003 1 11 1995 2 DM M 37 ## 6 22042 2 4 1995 2 DM F 36 ## 7 22044 2 4 1995 2 DM M 37 ## 8 22105 3 4 1995 2 DM F 37 ## 9 22109 3 4 1995 2 DM M 37 ## 10 22168 4 1 1995 2 DM M 36 ## # ... with 1,170 more rows, and 5 more variables: weight &lt;int&gt;, ## # genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt; 6.3 Pipes But what if you wanted to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, or pipes. With intermediate steps, you essentially create a temporary data frame and use that as input to the next function. This can clutter up your workspace with lots of objects. You can also nest functions (i.e. one function inside of another). This is handy, but can be difficult to read if too many functions are nested as things are evaluated from the inside out. The last option, pipes, are a fairly recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like %&gt;% and are made available via the magrittr package, installed automatically with dplyr. If you use RStudio, you can type the pipe with Ctrl + Shift + M if you have a PC or Cmd + Shift + M if you have a Mac. surveys %&gt;% filter(weight &lt; 5) %&gt;% select(species_id, sex, weight) ## # A tibble: 17 x 3 ## species_id sex weight ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 PF F 4 ## 2 PF F 4 ## 3 PF M 4 ## 4 RM F 4 ## 5 RM M 4 ## 6 PF &lt;NA&gt; 4 ## 7 PP M 4 ## 8 RM M 4 ## 9 RM M 4 ## 10 RM M 4 ## 11 PF M 4 ## 12 PF F 4 ## 13 RM M 4 ## 14 RM M 4 ## 15 RM F 4 ## 16 RM M 4 ## 17 RM M 4 In the above, we use the pipe to send the surveys dataset first through filter() to keep rows where weight is less than 5, then through select() to keep only the species_id, sex, and weight columns. Since %&gt;% takes the object on its left and passes it as the first argument to the function on its right, we don’t need to explicitly include it as an argument to the filter() and select() functions anymore. If we wanted to create a new object with this smaller version of the data, we could do so by assigning it a new name: surveys_sml &lt;- surveys %&gt;% filter(weight &lt; 5) %&gt;% select(species_id, sex, weight) surveys_sml ## # A tibble: 17 x 3 ## species_id sex weight ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 PF F 4 ## 2 PF F 4 ## 3 PF M 4 ## 4 RM F 4 ## 5 RM M 4 ## 6 PF &lt;NA&gt; 4 ## 7 PP M 4 ## 8 RM M 4 ## 9 RM M 4 ## 10 RM M 4 ## 11 PF M 4 ## 12 PF F 4 ## 13 RM M 4 ## 14 RM M 4 ## 15 RM F 4 ## 16 RM M 4 ## 17 RM M 4 Note that the final data frame is the leftmost part of this expression. Challenge {.challenge} Using pipes, subset the surveys data to include individuals collected before 1995 and retain only the columns year, sex, and weight. ## Answer surveys %&gt;% filter(year &lt; 1995) %&gt;% select(year, sex, weight) 6.3.1 Mutate Frequently you’ll want to create new columns based on the values in existing columns, for example to do unit conversions, or find the ratio of values in two columns. For this we’ll use mutate(). To create a new column of weight in kg: surveys %&gt;% mutate(weight_kg = weight / 1000) ## # A tibble: 34,786 x 14 ## record_id month day year plot_id species_id sex hindfoot_length ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 7 16 1977 2 NL M 32 ## 2 72 8 19 1977 2 NL M 31 ## 3 224 9 13 1977 2 NL &lt;NA&gt; NA ## 4 266 10 16 1977 2 NL &lt;NA&gt; NA ## 5 349 11 12 1977 2 NL &lt;NA&gt; NA ## 6 363 11 12 1977 2 NL &lt;NA&gt; NA ## 7 435 12 10 1977 2 NL &lt;NA&gt; NA ## 8 506 1 8 1978 2 NL &lt;NA&gt; NA ## 9 588 2 18 1978 2 NL M NA ## 10 661 3 11 1978 2 NL &lt;NA&gt; NA ## # ... with 34,776 more rows, and 6 more variables: weight &lt;int&gt;, ## # genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;, ## # weight_kg &lt;dbl&gt; You can also create a second new column based on the first new column within the same call of mutate(): surveys %&gt;% mutate(weight_kg = weight / 1000, weight_kg2 = weight_kg * 2) ## # A tibble: 34,786 x 15 ## record_id month day year plot_id species_id sex hindfoot_length ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 7 16 1977 2 NL M 32 ## 2 72 8 19 1977 2 NL M 31 ## 3 224 9 13 1977 2 NL &lt;NA&gt; NA ## 4 266 10 16 1977 2 NL &lt;NA&gt; NA ## 5 349 11 12 1977 2 NL &lt;NA&gt; NA ## 6 363 11 12 1977 2 NL &lt;NA&gt; NA ## 7 435 12 10 1977 2 NL &lt;NA&gt; NA ## 8 506 1 8 1978 2 NL &lt;NA&gt; NA ## 9 588 2 18 1978 2 NL M NA ## 10 661 3 11 1978 2 NL &lt;NA&gt; NA ## # ... with 34,776 more rows, and 7 more variables: weight &lt;int&gt;, ## # genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;, ## # weight_kg &lt;dbl&gt;, weight_kg2 &lt;dbl&gt; If this runs off your screen and you just want to see the first few rows, you can use a pipe to view the head() of the data. (Pipes work with non-dplyr functions, too, as long as the dplyr or magrittr package is loaded). surveys %&gt;% mutate(weight_kg = weight / 1000) %&gt;% head ## # A tibble: 6 x 14 ## record_id month day year plot_id species_id sex hindfoot_length ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 7 16 1977 2 NL M 32 ## 2 72 8 19 1977 2 NL M 31 ## 3 224 9 13 1977 2 NL &lt;NA&gt; NA ## 4 266 10 16 1977 2 NL &lt;NA&gt; NA ## 5 349 11 12 1977 2 NL &lt;NA&gt; NA ## 6 363 11 12 1977 2 NL &lt;NA&gt; NA ## # ... with 6 more variables: weight &lt;int&gt;, genus &lt;chr&gt;, species &lt;chr&gt;, ## # taxa &lt;chr&gt;, plot_type &lt;chr&gt;, weight_kg &lt;dbl&gt; Note that we don’t include parentheses at the end of our call to head() above. When piping into a function with no additional arguments, you can call the function with or without parentheses (e.g. head or head()). The first few rows of the output are full of NAs, so if we wanted to remove those we could insert a filter() in the chain: surveys %&gt;% filter(!is.na(weight)) %&gt;% mutate(weight_kg = weight / 1000) %&gt;% head ## # A tibble: 6 x 14 ## record_id month day year plot_id species_id sex hindfoot_length ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 588 2 18 1978 2 NL M NA ## 2 845 5 6 1978 2 NL M 32 ## 3 990 6 9 1978 2 NL M NA ## 4 1164 8 5 1978 2 NL M 34 ## 5 1261 9 4 1978 2 NL M 32 ## 6 1453 11 5 1978 2 NL M NA ## # ... with 6 more variables: weight &lt;int&gt;, genus &lt;chr&gt;, species &lt;chr&gt;, ## # taxa &lt;chr&gt;, plot_type &lt;chr&gt;, weight_kg &lt;dbl&gt; is.na() is a function that determines whether something is an NA. The ! symbol negates the result, so we’re asking for everything that is not an NA. Challenge {.challenge} Create a new data frame from the surveys data that meets the following criteria: contains only the species_id column and a new column called hindfoot_half containing values that are half the hindfoot_length values. In this hindfoot_half column, there are no NAs and all values are less than 30. Hint: think about how the commands should be ordered to produce this data frame! surveys_hindfoot_half &lt;- surveys %&gt;% filter(!is.na(hindfoot_length)) %&gt;% mutate(hindfoot_half = hindfoot_length / 2) %&gt;% filter(hindfoot_half &lt; 30) %&gt;% select(species_id, hindfoot_half) 6.3.2 Split-apply-combine data analysis and the summarize() function Many data analysis tasks can be approached using the split-apply-combine paradigm: split the data into groups, apply some analysis to each group, and then combine the results. dplyr makes this very easy through the use of the group_by() function. 6.3.2.1 The summarize() function group_by() is often used together with summarize(), which collapses each group into a single-row summary of that group. group_by() takes as arguments the column names that contain the categorical variables for which you want to calculate the summary statistics. So to view the mean weight by sex: surveys %&gt;% group_by(sex) %&gt;% summarize(mean_weight = mean(weight, na.rm = TRUE)) ## # A tibble: 3 x 2 ## sex mean_weight ## &lt;chr&gt; &lt;dbl&gt; ## 1 F 42.17055 ## 2 M 42.99538 ## 3 &lt;NA&gt; 64.74257 You may also have noticed that the output from these calls doesn’t run off the screen anymore. It’s one of the advantages of tbl_df over data frame. You can also group by multiple columns: surveys %&gt;% group_by(sex, species_id) %&gt;% summarize(mean_weight = mean(weight, na.rm = TRUE)) ## # A tibble: 92 x 3 ## # Groups: sex [?] ## sex species_id mean_weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 F BA 9.16129 ## 2 F DM 41.60968 ## 3 F DO 48.53125 ## 4 F DS 117.74955 ## 5 F NL 154.28221 ## 6 F OL 31.06582 ## 7 F OT 24.83090 ## 8 F OX 21.00000 ## 9 F PB 30.21088 ## 10 F PE 22.82218 ## # ... with 82 more rows When grouping both by sex and species_id, the first rows are for individuals that escaped before their sex could be determined and weighted. You may notice that the last column does not contain NA but NaN (which refers to “Not a Number”). To avoid this, we can remove the missing values for weight before we attempt to calculate the summary statistics on weight. Because the missing values are removed, we can omit na.rm = TRUE when computing the mean: surveys %&gt;% filter(!is.na(weight)) %&gt;% group_by(sex, species_id) %&gt;% summarize(mean_weight = mean(weight)) ## # A tibble: 64 x 3 ## # Groups: sex [?] ## sex species_id mean_weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 F BA 9.16129 ## 2 F DM 41.60968 ## 3 F DO 48.53125 ## 4 F DS 117.74955 ## 5 F NL 154.28221 ## 6 F OL 31.06582 ## 7 F OT 24.83090 ## 8 F OX 21.00000 ## 9 F PB 30.21088 ## 10 F PE 22.82218 ## # ... with 54 more rows Here, again, the output from these calls doesn’t run off the screen anymore. If you want to display more data, you can use the print() function at the end of your chain with the argument n specifying the number of rows to display: surveys %&gt;% filter(!is.na(weight)) %&gt;% group_by(sex, species_id) %&gt;% summarize(mean_weight = mean(weight)) %&gt;% print(n = 15) ## # A tibble: 64 x 3 ## # Groups: sex [?] ## sex species_id mean_weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 F BA 9.161290 ## 2 F DM 41.609685 ## 3 F DO 48.531250 ## 4 F DS 117.749548 ## 5 F NL 154.282209 ## 6 F OL 31.065817 ## 7 F OT 24.830904 ## 8 F OX 21.000000 ## 9 F PB 30.210884 ## 10 F PE 22.822183 ## 11 F PF 7.974394 ## 12 F PH 30.850000 ## 13 F PL 19.312500 ## 14 F PM 22.125668 ## 15 F PP 17.180670 ## # ... with 49 more rows Once the data are grouped, you can also summarize multiple variables at the same time (and not necessarily on the same variable). For instance, we could add a column indicating the minimum weight for each species for each sex: surveys %&gt;% filter(!is.na(weight)) %&gt;% group_by(sex, species_id) %&gt;% summarize(mean_weight = mean(weight), min_weight = min(weight)) ## # A tibble: 64 x 4 ## # Groups: sex [?] ## sex species_id mean_weight min_weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 F BA 9.16129 6 ## 2 F DM 41.60968 10 ## 3 F DO 48.53125 12 ## 4 F DS 117.74955 45 ## 5 F NL 154.28221 32 ## 6 F OL 31.06582 10 ## 7 F OT 24.83090 5 ## 8 F OX 21.00000 20 ## 9 F PB 30.21088 12 ## 10 F PE 22.82218 11 ## # ... with 54 more rows 6.3.2.2 Tallying When working with data, it is also common to want to know the number of observations found for each factor or combination of factors. For this, dplyr provides tally(). For example, if we wanted to group by sex and find the number of rows of data for each sex, we would do: surveys %&gt;% group_by(sex) %&gt;% tally ## # A tibble: 3 x 2 ## sex n ## &lt;chr&gt; &lt;int&gt; ## 1 F 15690 ## 2 M 17348 ## 3 &lt;NA&gt; 1748 Here, tally() is the action applied to the groups created by group_by() and counts the total number of records for each category. Challenge {.challenge} How many individuals were caught in each plot_type surveyed? Use group_by() and summarize() to find the mean, min, and max hindfoot length for each species (using species_id). What was the heaviest animal measured in each year? Return the columns year, genus, species_id, and weight. You saw above how to count the number of individuals of each sex using a combination of group_by() and tally(). How could you get the same result using group_by() and summarize()? Hint: see ?n. ## # A tibble: 5 x 2 ## plot_type n ## &lt;chr&gt; &lt;int&gt; ## 1 Control 15611 ## 2 Long-term Krat Exclosure 5118 ## 3 Rodent Exclosure 4233 ## 4 Short-term Krat Exclosure 5906 ## 5 Spectab exclosure 3918 ## # A tibble: 25 x 4 ## species_id mean_hindfoot_length min_hindfoot_length max_hindfoot_length ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AH 33.00000 31 35 ## 2 BA 13.00000 6 16 ## 3 DM 35.98235 16 50 ## 4 DO 35.60755 26 64 ## 5 DS 49.94887 39 58 ## 6 NL 32.29423 21 70 ## 7 OL 20.53261 12 39 ## 8 OT 20.26741 13 50 ## 9 OX 19.12500 13 21 ## 10 PB 26.11592 2 47 ## # ... with 15 more rows ## # A tibble: 27 x 4 ## # Groups: year [26] ## year genus species weight ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 1977 Dipodomys spectabilis 149 ## 2 1978 Neotoma albigula 232 ## 3 1978 Neotoma albigula 232 ## 4 1979 Neotoma albigula 274 ## 5 1980 Neotoma albigula 243 ## 6 1981 Neotoma albigula 264 ## 7 1982 Neotoma albigula 252 ## 8 1983 Neotoma albigula 256 ## 9 1984 Neotoma albigula 259 ## 10 1985 Neotoma albigula 225 ## # ... with 17 more rows ## # A tibble: 3 x 2 ## sex n ## &lt;chr&gt; &lt;int&gt; ## 1 F 15690 ## 2 M 17348 ## 3 &lt;NA&gt; 1748 6.3.3 Reshaping with gather and spread In the spreadsheet lesson we discussed how to structure our data leading to the four rules defining a tidy dataset: Each variable has its own column Each observation has its own row Each value must have its own cell Each type of observational unit forms a table Here we examine the fourth rule: Each type of observational unit forms a table. In surveys , the rows of surveys contain the values of variables associated with each record (the unit), values such the weight or sex of each animal associated with each record. What if instead of comparing records, we wanted to compare the different mean weight of each species between plots? (Ignoring plot_type for simplicity). We’d need to create a new table where each row (the unit) comprises of values of variables associated with each plot. In practical terms this means the values of the species in genus would become the names of column variables and the cells would contain the values of the mean weight observed on each plot. Having created a new table, it is therefore straightforward to explore the relationship between the weight of different species within, and between, the plots. The key point here is that we are still following a tidy data structure, but we have reshaped the data according to the observations of interest: average species weight per plot instead of recordings per date. The opposite transformation would be to transform column names into values of a variable. We can do both these of transformations with two tidyr functions, spread() and gather(). 6.3.3.1 Spreading spread() takes three principal arguments: the data the key column variable whose values will become new column names. the value column variable whose values will fill the new column variables. Further arguements include fill which if set, fills in missing values with that provided. Let’s use spread() to transform surveys to find the mean weight of each species in each plot over the entire survey period. We use filter(), group_by() and summarise() to filter our observations and variables of interest, and create a new variable for the mean_weight. We use the pipe as before too. surveys_gw &lt;- surveys %&gt;% filter(!is.na(weight)) %&gt;% group_by(genus, plot_id) %&gt;% summarize(mean_weight = mean(weight)) str(surveys_gw) ## Classes &#39;grouped_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 196 obs. of 3 variables: ## $ genus : chr &quot;Baiomys&quot; &quot;Baiomys&quot; &quot;Baiomys&quot; &quot;Baiomys&quot; ... ## $ plot_id : int 1 2 3 5 18 19 20 21 1 2 ... ## $ mean_weight: num 7 6 8.61 7.75 9.5 ... ## - attr(*, &quot;spec&quot;)=List of 2 ## ..$ cols :List of 13 ## .. ..$ record_id : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ month : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ day : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ year : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ plot_id : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ species_id : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ sex : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ hindfoot_length: list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ weight : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ genus : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ species : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ taxa : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ plot_type : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## ..$ default: list() ## .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_guess&quot; &quot;collector&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;col_spec&quot; ## - attr(*, &quot;vars&quot;)= chr &quot;genus&quot; ## - attr(*, &quot;drop&quot;)= logi TRUE This yields surveys_gw where the observation for each plot is spread across multiple rows, 196 observations of 13 variables. Using spread() to key on genus with values from mean_weight this becomes 24 observations of 11 variables, one row for each plot. We use the pipe as before too. surveys_spread &lt;- surveys_gw %&gt;% spread(key = genus, value = mean_weight) str(surveys_spread) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 24 obs. of 11 variables: ## $ plot_id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Baiomys : num 7 6 8.61 NA 7.75 ... ## $ Chaetodipus : num 22.2 25.1 24.6 23 18 ... ## $ Dipodomys : num 60.2 55.7 52 57.5 51.1 ... ## $ Neotoma : num 156 169 158 164 190 ... ## $ Onychomys : num 27.7 26.9 26 28.1 27 ... ## $ Perognathus : num 9.62 6.95 7.51 7.82 8.66 ... ## $ Peromyscus : num 22.2 22.3 21.4 22.6 21.2 ... ## $ Reithrodontomys: num 11.4 10.7 10.5 10.3 11.2 ... ## $ Sigmodon : num NA 70.9 65.6 82 82.7 ... ## $ Spermophilus : num NA NA NA NA NA NA NA NA NA NA ... We could now plot comparisons between the weight of species on different plots for example, although we may wish to fill in the missing values first. surveys_gw %&gt;% spread(genus, mean_weight, fill = 0) %&gt;% head() ## # A tibble: 6 x 11 ## plot_id Baiomys Chaetodipus Dipodomys Neotoma Onychomys Perognathus ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 7.000000 22.19939 60.23214 156.2222 27.67550 9.625000 ## 2 2 6.000000 25.11014 55.68259 169.1436 26.87302 6.947368 ## 3 3 8.611111 24.63636 52.04688 158.2414 26.03241 7.507812 ## 4 4 0.000000 23.02381 57.52454 164.1667 28.09375 7.824427 ## 5 5 7.750000 17.98276 51.11356 190.0370 27.01695 8.658537 ## 6 6 0.000000 24.86009 58.60531 179.9333 25.89947 7.809524 ## # ... with 4 more variables: Peromyscus &lt;dbl&gt;, Reithrodontomys &lt;dbl&gt;, ## # Sigmodon &lt;dbl&gt;, Spermophilus &lt;dbl&gt; 6.3.3.2 Gathering The opposing situation could occur if we had been provided with data in the form of surveys_spread, where the genus names are column names, but we wish to treat them as values of a genus variable instead. In this situation we are gathering the column names and turning them into a pair of new variables. One variable represents the column names as values, and the other variable contains the values previously associated with the column names. gather() takes four principal arguments: the data the key column variable we wish to create from column names. the values column variable we wish to create and fill with values associated with the key. the names of the columns we use to fill the key variable (or to drop). To recreate surveys_gw from surveys_spread we would create a key called genus and value called mean_weight and use all columns except plot_id for the key variable. Here we drop plot_id column with a minus sign. surveys_gather &lt;- surveys_spread %&gt;% gather(key = genus, value = mean_weight, -plot_id) str(surveys_gather) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 240 obs. of 3 variables: ## $ plot_id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ genus : chr &quot;Baiomys&quot; &quot;Baiomys&quot; &quot;Baiomys&quot; &quot;Baiomys&quot; ... ## $ mean_weight: num 7 6 8.61 NA 7.75 ... Note that now the NA genera are included in the re-gathered format. Spreading and then gathering can be a useful way to balance out a dataset so every replicate has the same composition. We could also have used a specification for what columns to include. This can be useful if you have a large number of identifying columns, and it’s easier to specify what to gather than what to leave alone. And if the columns are in a row, we don’t even need to list them all out - just use the : operator! surveys_spread %&gt;% gather(key = genus, value = mean_weight, Baiomys:Spermophilus) %&gt;% head() ## # A tibble: 6 x 3 ## plot_id genus mean_weight ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Baiomys 7.000000 ## 2 2 Baiomys 6.000000 ## 3 3 Baiomys 8.611111 ## 4 4 Baiomys NA ## 5 5 Baiomys 7.750000 ## 6 6 Baiomys NA Challenge {.challenge} Spread the data frame with year as columns, plot_id as rows, and the values are the number of genera per plot. You will need to summarize before reshaping, and use the function n_distinct() to get the number of unique types of a genera. It’s a powerful function! See ?n_distinct for more. Now take that data frame, and make gather it again, so each row is a unique plot_id year combination. The surveys data set has two columns of measurement: hindfoot_length and weight. This makes it difficult to do things like look at the relationship between mean values of each measurement per year in different plot types. Let’s walk through a common solution for this type of problem. First, use gather() to create a dataset where we have a key column called measurement and a value column that takes on the value of either hindfoot_length or weight. Hint: You’ll need to specify which columns are being gathered. With this new data set, calculate the average of each measurement in each year &gt; for each different plot_type. Then spread() them into a data set with a column for hindfoot_length and weight. Hint: Remember, you only need to specify the key and value columns for spread(). ## # A tibble: 6 x 27 ## # Groups: plot_id [6] ## plot_id `1977` `1978` `1979` `1980` `1981` `1982` `1983` `1984` `1985` ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 2 3 4 7 5 6 7 6 4 ## 2 2 6 6 6 8 5 9 9 9 6 ## 3 3 5 6 4 6 6 8 10 11 7 ## 4 4 4 4 3 4 5 4 6 3 4 ## 5 5 4 3 2 5 4 6 7 7 3 ## 6 6 3 4 3 4 5 9 9 7 5 ## # ... with 17 more variables: `1986` &lt;int&gt;, `1987` &lt;int&gt;, `1988` &lt;int&gt;, ## # `1989` &lt;int&gt;, `1990` &lt;int&gt;, `1991` &lt;int&gt;, `1992` &lt;int&gt;, `1993` &lt;int&gt;, ## # `1994` &lt;int&gt;, `1995` &lt;int&gt;, `1996` &lt;int&gt;, `1997` &lt;int&gt;, `1998` &lt;int&gt;, ## # `1999` &lt;int&gt;, `2000` &lt;int&gt;, `2001` &lt;int&gt;, `2002` &lt;int&gt; ## # A tibble: 624 x 3 ## # Groups: plot_id [24] ## plot_id year n_genera ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 1977 2 ## 2 2 1977 6 ## 3 3 1977 5 ## 4 4 1977 4 ## 5 5 1977 4 ## 6 6 1977 3 ## 7 7 1977 3 ## 8 8 1977 2 ## 9 9 1977 3 ## 10 10 1977 1 ## # ... with 614 more rows ## # A tibble: 130 x 4 ## # Groups: year [26] ## year plot_type hindfoot_length weight ## * &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1977 Control 36.13483 50.44094 ## 2 1977 Long-term Krat Exclosure 33.70833 34.78947 ## 3 1977 Rodent Exclosure 39.10000 48.20000 ## 4 1977 Short-term Krat Exclosure 35.84884 41.27119 ## 5 1977 Spectab exclosure 37.23913 47.12903 ## 6 1978 Control 38.06467 70.75169 ## 7 1978 Long-term Krat Exclosure 22.56667 35.93548 ## 8 1978 Rodent Exclosure 37.83077 67.27536 ## 9 1978 Short-term Krat Exclosure 36.87432 63.80347 ## 10 1978 Spectab exclosure 42.28448 80.13223 ## # ... with 120 more rows 6.4 Exporting data Now that you have learned how to use dplyr to extract information from or summarize your raw data, you may want to export these new datasets to share them with your collaborators or for archival. Similar to the read_csv() function used for reading CSV files into R, there is a write_csv() function that generates CSV files from data frames. Before using write_csv(), we are going to create a new folder, data_output, in our working directory that will store this generated dataset. We don’t want to write generated datasets in the same directory as our raw data. It’s good practice to keep them separate. The data folder should only contain the raw, unaltered data, and should be left alone to make sure we don’t delete or modify it. In contrast, our script will generate the contents of the data_output directory, so even if the files it contains are deleted, we can always re-generate them. In preparation for our next lesson on plotting, we are going to prepare a cleaned up version of the dataset that doesn’t include any missing data. Let’s start by removing observations for which the species_id is missing. In this dataset, the missing species are represented by an empty string and not an NA. Let’s also remove observations for which weight and the hindfoot_length are missing. This dataset should also only contain observations of animals for which the sex has been determined: surveys_complete &lt;- surveys %&gt;% filter(species_id != &quot;&quot;, # remove missing species_id !is.na(weight), # remove missing weight !is.na(hindfoot_length), # remove missing hindfoot_length sex != &quot;&quot;) # remove missing sex Because we are interested in plotting how species abundances have changed through time, we are also going to remove observations for rare species (i.e., that have been observed less than 50 times). We will do this in two steps: first we are going to create a dataset that counts how often each species has been observed, and filter out the rare species; then, we will extract only the observations for these more common species: ## Extract the most common species_id species_counts &lt;- surveys_complete %&gt;% group_by(species_id) %&gt;% tally %&gt;% filter(n &gt;= 50) ## Only keep the most common species surveys_complete &lt;- surveys_complete %&gt;% filter(species_id %in% species_counts$species_id) To make sure that everyone has the same dataset, check that surveys_complete has 30463 rows and 13 columns by typing dim(surveys_complete). Now that our dataset is ready, we can save it as a CSV file in our data_output folder. write_csv(surveys_complete, path = &quot;data/surveys_complete.csv&quot;) "],
["data-visualization-with-ggplot2.html", "Chapter 7 Data visualization with ggplot2 7.1 Plotting with ggplot2 7.2 Building your plots iteratively 7.3 Boxplot 7.4 Plotting time series data 7.5 Faceting 7.6 ggplot2 themes 7.7 Customization 7.8 Arranging and exporting plots", " Chapter 7 Data visualization with ggplot2 The following material is based on Data Carpentry’s the Data analisis and visualisation lessons. Learning Objectives: Produce scatter plots, boxplots, and time series plots using ggplot. Set universal plot settings. Understand and apply faceting in ggplot. Modify the aesthetics of an existing ggplot plot (including axis labels and color). Build complex and customized plots from data in a data frame. We start by loading the required packages. ggplot2 is included in the tidyverse package. library(tidyverse) If not still in the workspace, load the data we saved in the previous lesson. surveys_complete &lt;- read_csv(&#39;data/surveys_complete.csv&#39;) 7.1 Plotting with ggplot2 ggplot2 is a plotting package that makes it simple to create complex plots from data in a data frame. It provides a more programmatic interface for specifying what variables to plot, how they are displayed, and general visual properties. Therefore, we only need minimal changes if the underlying data change or if we decide to change from a bar plot to a scatterplot. This helps in creating publication quality plots with minimal amounts of adjustments and tweaking. ggplot likes data in the ‘long’ format: i.e., a column for every dimension, and a row for every observation. Well structured data will save you lots of time when making figures with ggplot. ggplot graphics are built step by step by adding new elements. Adding layers in this fashion allows for extensive flexibility and customization of plots. To build a ggplot, we need to: use the ggplot() function and bind the plot to a specific data frame using the data argument ggplot(data = surveys_complete) define aesthetics (aes), by selecting the variables to be plotted and the variables to define the presentation such as plotting size, shape color, etc. ggplot(data = surveys_complete, aes(x = weight, y = hindfoot_length)) add geoms – graphical representation of the data in the plot (points, lines, bars). ggplot2 offers many different geoms; we will use some common ones today, including: geom_point() for scatter plots, dot plots, etc. geom_boxplot() for, well, boxplots! geom_line() for trend lines, time-series, etc. To add a geom to the plot use + operator. Because we have two continuous variables, let’s use geom_point() first: ggplot(data = surveys_complete, aes(x = weight, y = hindfoot_length)) + geom_point() The + in the ggplot2 package is particularly useful because it allows you to modify existing ggplot objects. This means you can easily set up plot “templates” and conveniently explore different types of plots, so the above plot can also be generated with code like this: # Assign plot to a variable surveys_plot &lt;- ggplot(data = surveys_complete, aes(x = weight, y = hindfoot_length)) # Draw the plot surveys_plot + geom_point() Notes: Anything you put in the ggplot() function can be seen by any geom layers that you add (i.e., these are universal plot settings). This includes the x and y axis you set up in aes(). You can also specify aesthetics for a given geom independently of the aesthetics defined globally in the ggplot() function. The + sign used to add layers must be placed at the end of each line containing a layer. If, instead, the + sign is added in the line before the other layer, ggplot2 will not add the new layer and will return an error message. # this is the correct syntax for adding layers surveys_plot + geom_point() # this will not add the new layer and will return an error message surveys_plot + geom_point() 7.1 Challenge (optional) Scatter plots can be useful exploratory tools for small datasets. For data sets with large numbers of observations, such as the surveys_complete data set, overplotting of points can be a limitation of scatter plots. One strategy for handling such settings is to use hexagonal binning of observations. The plot space is tessellated into hexagons. Each hexagon is assigned a color based on the number of observations that fall within its boundaries. To use hexagonal binning with ggplot2, first install the R package hexbin from CRAN: install.packages(&quot;hexbin&quot;) library(hexbin) Then use the geom_hex() function: surveys_plot + geom_hex() What are the relative strengths and weaknesses of a hexagonal bin plot compared to a scatter plot? Examine the above scatter plot and compare it with the hexagonal bin plot that you created. 7.2 Building your plots iteratively Building plots with ggplot is typically an iterative process. We start by defining the dataset we’ll use, lay the axes, and choose a geom: ggplot(data = surveys_complete, aes(x = weight, y = hindfoot_length)) + geom_point() Then, we start modifying this plot to extract more information from it. For instance, we can add transparency (alpha) to avoid overplotting: ggplot(data = surveys_complete, aes(x = weight, y = hindfoot_length)) + geom_point(alpha = 0.1) We can also add colors for all the points: ggplot(data = surveys_complete, aes(x = weight, y = hindfoot_length)) + geom_point(alpha = 0.1, color = &quot;blue&quot;) Or to color each species in the plot differently: ggplot(data = surveys_complete, aes(x = weight, y = hindfoot_length)) + geom_point(alpha = 0.1, aes(color=species_id)) Challenge Use what you just learned to create a scatter plot of weight over species_id with the plot types showing in different colors. Is this a good way to show this type of data? 7.3 Boxplot We can use boxplots to visualize the distribution of weight within each species: ggplot(data = surveys_complete, aes(x = species_id, y = weight)) + geom_boxplot() By adding points to boxplot, we can have a better idea of the number of measurements and of their distribution: ggplot(data = surveys_complete, aes(x = species_id, y = weight)) + geom_boxplot(alpha = 0) + geom_jitter(alpha = 0.3, color = &quot;tomato&quot;) Notice how the boxplot layer is behind the jitter layer? What do you need to change in the code to put the boxplot in front of the points such that it’s not hidden? Challenges Boxplots are useful summaries, but hide the shape of the distribution. For example, if there is a bimodal distribution, it would not be observed with a boxplot. An alternative to the boxplot is the violin plot (sometimes known as a beanplot), where the shape (of the density of points) is drawn. Replace the box plot with a violin plot; see geom_violin(). In many types of data, it is important to consider the scale of the observations. For example, it may be worth changing the scale of the axis to better distribute the observations in the space of the plot. Changing the scale of the axes is done similarly to adding/modifying other components (i.e., by incrementally adding commands). Try making these modifications: Represent weight on the log10 scale; see scale_y_log10(). So far, we’ve looked at the distribution of weight within species. Try making a new plot to explore the distribution of another variable within each species. Create boxplot for hindfoot_length. Overlay the boxplot layer on a jitter layer to show actual measurements. Add color to the datapoints on your boxplot according to the plot from which the sample was taken (plot_id). Hint: Check the class for plot_id. Consider changing the class of plot_id from integer to factor. Why does this change how R makes the graph? ## Challenge with boxplots: ## Start with the boxplot we created: ggplot(data = surveys_complete, aes(x = species_id, y = weight)) + geom_boxplot(alpha = 0) + geom_jitter(alpha = 0.3, color = &quot;tomato&quot;) ## 1. Replace the box plot with a violin plot; see `geom_violin()`. ## 2. Represent weight on the log10 scale; see `scale_y_log10()`. ## 3. Create boxplot for `hindfoot_length` overlaid on a jitter layer. ## 4. Add color to the datapoints on your boxplot according to the ## plot from which the sample was taken (`plot_id`). ## Hint: Check the class for `plot_id`. Consider changing the class ## of `plot_id` from integer to factor. Why does this change how R ## makes the graph? 7.4 Plotting time series data Let’s calculate number of counts per year for each species. First we need to group the data and count records within each group: yearly_counts &lt;- surveys_complete %&gt;% group_by(year, species_id) %&gt;% tally() Timelapse data can be visualized as a line plot with years on the x axis and counts on the y axis: ggplot(data = yearly_counts, aes(x = year, y = n)) + geom_line() Unfortunately, this does not work because we plotted data for all the species together. We need to tell ggplot to draw a line for each species by modifying the aesthetic function to include group = species_id: ggplot(data = yearly_counts, aes(x = year, y = n, group = species_id)) + geom_line() We will be able to distinguish species in the plot if we add colors (using color also automatically groups the data): ggplot(data = yearly_counts, aes(x = year, y = n, color = species_id)) + geom_line() 7.5 Faceting ggplot has a special technique called faceting that allows the user to split one plot into multiple plots based on a factor included in the dataset. We will use it to make a time series plot for each species: ggplot(data = yearly_counts, aes(x = year, y = n)) + geom_line() + facet_wrap(~ species_id) Now we would like to split the line in each plot by the sex of each individual measured. To do that we need to make counts in the data frame grouped by year, species_id, and sex: yearly_sex_counts &lt;- surveys_complete %&gt;% group_by(year, species_id, sex) %&gt;% tally() We can now make the faceted plot by splitting further by sex using color (within a single plot): ggplot(data = yearly_sex_counts, aes(x = year, y = n, color = sex)) + geom_line() + facet_wrap(~ species_id) Usually plots with white background look more readable when printed. We can set the background to white using the function theme_bw(). Additionally, you can remove the grid: ggplot(data = yearly_sex_counts, aes(x = year, y = n, color = sex)) + geom_line() + facet_wrap(~ species_id) + theme_bw() + theme(panel.grid = element_blank()) 7.6 ggplot2 themes In addition to theme_bw(), which changes the plot background to white, ggplot2 comes with several other themes which can be useful to quickly change the look of your visualization. The complete list of themes is available at http://docs.ggplot2.org/current/ggtheme.html. theme_minimal() and theme_light() are popular, and theme_void() can be useful as a starting point to create a new hand-crafted theme. The ggthemes package provides a wide variety of options (including an Excel 2003 theme). The ggplot2 extensions website provides a list of packages that extend the capabilities of ggplot2, including additional themes. Challenge Use what you just learned to create a plot that depicts how the average weight of each species changes through the years. yearly_weight &lt;- surveys_complete %&gt;% group_by(year, species_id) %&gt;% summarize(avg_weight = mean(weight)) ggplot(data = yearly_weight, aes(x=year, y=avg_weight)) + geom_line() + facet_wrap(~ species_id) + theme_bw() The facet_wrap geometry extracts plots into an arbitrary number of dimensions to allow them to cleanly fit on one page. On the other hand, the facet_grid geometry allows you to explicitly specify how you want your plots to be arranged via formula notation (rows ~ columns; a . can be used as a placeholder that indicates only one row or column). Let’s modify the previous plot to compare how the weights of males and females has changed through time: # One column, facet by rows yearly_sex_weight &lt;- surveys_complete %&gt;% group_by(year, sex, species_id) %&gt;% summarize(avg_weight = mean(weight)) ggplot(data = yearly_sex_weight, aes(x=year, y=avg_weight, color = species_id)) + geom_line() + facet_grid(sex ~ .) # One row, facet by column ggplot(data = yearly_sex_weight, aes(x=year, y=avg_weight, color = species_id)) + geom_line() + facet_grid(. ~ sex) 7.7 Customization Take a look at the ggplot2 cheat sheet, and think of ways you could improve the plot. Now, let’s change names of axes to something more informative than ‘year’ and ‘n’ and add a title to the figure: ggplot(data = yearly_sex_counts, aes(x = year, y = n, color = sex)) + geom_line() + facet_wrap(~ species_id) + labs(title = &#39;Observed species in time&#39;, x = &#39;Year of observation&#39;, y = &#39;Number of species&#39;) + theme_bw() The axes have more informative names, but their readability can be improved by increasing the font size: ggplot(data = yearly_sex_counts, aes(x = year, y = n, color = sex)) + geom_line() + facet_wrap(~ species_id) + labs(title = &#39;Observed species in time&#39;, x = &#39;Year of observation&#39;, y = &#39;Number of species&#39;) + theme_bw() + theme(text=element_text(size=16)) Note that it is also possible to change the fonts of your plots. If you are on Windows, you may have to install the extrafont package, and follow the instructions included in the README for this package. After our manipulations, you may notice that the values on the x-axis are still not properly readable. Let’s change the orientation of the labels and adjust them vertically and horizontally so they don’t overlap. You can use a 90 degree angle, or experiment to find the appropriate angle for diagonally oriented labels: ggplot(data = yearly_sex_counts, aes(x = year, y = n, color = sex)) + geom_line() + facet_wrap(~ species_id) + labs(title = &#39;Observed species in time&#39;, x = &#39;Year of observation&#39;, y = &#39;Number of species&#39;) + theme_bw() + theme(axis.text.x = element_text(colour=&quot;grey20&quot;, size=12, angle=90, hjust=.5, vjust=.5), axis.text.y = element_text(colour=&quot;grey20&quot;, size=12), text=element_text(size=16)) If you like the changes you created better than the default theme, you can save them as an object to be able to easily apply them to other plots you may create: grey_theme &lt;- theme(axis.text.x = element_text(colour=&quot;grey20&quot;, size=12, angle=90, hjust=.5, vjust=.5), axis.text.y = element_text(colour=&quot;grey20&quot;, size=12), text=element_text(size=16)) ggplot(surveys_complete, aes(x = species_id, y = hindfoot_length)) + geom_boxplot() + grey_theme Challenge With all of this information in hand, please take another five minutes to either improve one of the plots generated in this exercise or create a beautiful graph of your own. Use the RStudio ggplot2 cheat sheet for inspiration. Here are some ideas: See if you can change the thickness of the lines. Can you find a way to change the name of the legend? What about its labels? Try using a different color palette (see http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/). 7.8 Arranging and exporting plots Faceting is a great tool for splitting one plot into multiple plots, but sometimes you may want to produce a single figure that contains multiple plots using different variables or even different data frames. The gridExtra package allows us to combine separate ggplots into a single figure using grid.arrange(): library(gridExtra) spp_weight_boxplot &lt;- ggplot(data = surveys_complete, aes(x = species_id, y = weight)) + geom_boxplot() + xlab(&quot;Species&quot;) + ylab(&quot;Weight (g)&quot;) + scale_y_log10() spp_count_plot &lt;- ggplot(data = yearly_counts, aes(x = year, y = n, color = species_id)) + geom_line() + xlab(&quot;Year&quot;) + ylab(&quot;Abundance&quot;) grid.arrange(spp_weight_boxplot, spp_count_plot, ncol = 2, widths = c(4,6)) In addition to the ncol and nrow arguments, used to make simple arrangements, there are tools for constucting more complex layouts. After creating your plot, you can save it to a file in your favorite format. The Export tab in the Plot pane in RStudio will save your plots at low resolution, which will not be accepted by many journals and will not scale well for posters. Instead, use the ggsave() function, which allows you easily change the dimension and resolution of your plot by adjusting the appropriate arguments (width, height and dpi): my_plot &lt;- ggplot(data = yearly_sex_counts, aes(x = year, y = n, color = sex)) + geom_line() + facet_wrap(~ species_id) + labs(title = &#39;Observed species in time&#39;, x = &#39;Year of observation&#39;, y = &#39;Number of species&#39;) + theme_bw() + theme(axis.text.x = element_text(colour=&quot;grey20&quot;, size=12, angle=90, hjust=.5, vjust=.5), axis.text.y = element_text(colour=&quot;grey20&quot;, size=12), text=element_text(size=16)) ggsave(&quot;name_of_file.png&quot;, my_plot, width=15, height=10) ## This also works for grid.arrange() plots combo_plot &lt;- grid.arrange(spp_weight_boxplot, spp_count_plot, ncol = 2, widths = c(4,6)) ggsave(&quot;combo_plot_abun_weight.png&quot;, combo_plot, width = 10, dpi = 300) Note: The parameters width and height also determine the font size in the saved plot. "],
["reproducible-research-with-rmarkdown.html", "Chapter 8 Reproducible research with Rmarkdown 8.1 Introduction 8.2 Additional features 8.3 Challenge", " Chapter 8 Reproducible research with Rmarkdown 8.1 Introduction Reproducible research is an essential part of any data analysis. With the tools that are available, one can argue that it has become more difficult not to produce reproducible reports than to producing then. Reproducible documents have been a part of R since the very beginning. See for example Statistical Analyses and Reproducible Research by Gentleman and Temple Land, 2004), to see how such compendia play a central role within the Bioconductor project (more about Bioconductor in it’s dedicated section). Originally, these were written in LaTeX, interleaved with R code chunks, forming so called Sweave documents (with extension .Rnw). More recently, it has become to use the markdown syntax markup language, rather than LaTeX. Once interleaved with R code chunks, these documents become Rmarkdown files (.Rmd). The can be converted into markdown using knitr::knit, that executes the code chunk and incorporates their output in the resulting markdown documents, which itself is converted to one of many output formats, typically pdf of html using pandoc. I R, this final conversion is done using rmarkdown::render (that relies on pandoc). The rmarkdown package is developed and maintained by RStudio and benefits from excellent documentation, support and integrates into the RStudio editor. RStudio now also support R Notebook documents that execute individual code chunks that are displayed directly in the source document. 8.2 Additional features Among the most options that can set for code chunks is cache. Setting cache=TRUE will avoid that specific code chunk to be cached and not recomputed every time the documented in knitted, unless the code chunk was modified. This is an important feature when long computations are necessary. The DT::datatable function allows to create dynamic tables directly from R, as show below. DT::datatable(cars) It is always useful to finish a Rmarkdown report with a section providing all the session information details with the sessionInfo() function, such at the end of this material. This allows readers to review the version of R itself and all the packages that were used to produce the report. 8.3 Challenge Prepare an Rmarkdown report summarising the portal ecology data. The report should include a Material and methods section where the data is read in (ideally from the online file) and briefly described, a Data preparation section where rows with missing values are filtered out, and a Visualisation section where one or two plots are rendered. Finish your report with a Session information section. "],
["r-package-development.html", "Chapter 9 R Package Development 9.1 Introduction 9.2 Preparing R code 9.3 Package layout 9.4 Package developement cycle 9.5 Package metadata 9.6 NAMESPACE 9.7 R code 9.8 Package sub-directories 9.9 Documentation 9.10 Manual pages 9.11 Additional files 9.12 Distributing packages", " Chapter 9 R Package Development This section is based on material from the https://github.com/lgatto/RPackageDevelopment course. 9.1 Introduction Packages are the way to share R code in a structures, reproducible and tractable way. Even if the intend is not to disseminate your code, packaging it is worth it. Packages provide a mechanism for loading optional code and attached documentation as needed. logically group your own functions keep code and documentation together and consistent keep code and data together keep track of changes in code summarise all packages used for a analysis (see sessionInfo()) make a reproducible research compendium (container for code, text, data as a means for distributing, managing and updating) optionally test your code … project managment 9.1.1 References R packages, by Hadley Wickham R Installation and Administration [R-admin], R Core team Writing R Extensions [R-ext], R Core team Use help.start() to access them from your local installation, or http://cran.r-project.org/manuals.html from the web. 9.1.2 Terminology A package is loaded from a library by the function library(). Thus a library is a directory containing installed packages. Calling library(&quot;foo&quot;, lib.loc = &quot;/path/to/bar&quot;) loads the package (book) foo from the library bar located at /path/to/bar. 9.1.3 Requirement library(&quot;devtools&quot;) library(&quot;roxygen2&quot;) 9.1.4 Course content Basic workflow Prepare R code Create package directory: mypackage Build the package tarball Check the package Install the package Step 2 is done only once. Package developement cycles through 3 - 5. Also - Writing package documentation - Vignettes - Testing packages - Compiled code 9.2 Preparing R code fn &lt;- function() message(&quot;I love R packages&quot;) 9.3 Package layout We can use package.skeleton(&quot;myRpackage&quot;, list = &quot;fn&quot;) devtools::create(&quot;myRpackage&quot;) also create an .Rproj file. Use the RStudio wizard: New Project &gt; New Directory &gt; R Package myRpackage/ |-- DESCRIPTION |-- NAMESPACE |-- man | `-- fun.Rd `-- R `-- fun.R This is the source package. From this, we need to create the package tarball (or package bundle), i.e. a compressed archive of the source. We can also create binary packages for Windows and Mac. 9.4 Package developement cycle In the shell R CMD build myPackage ## creates myRpackage_1.0.tar.gz R CMD check myPackage_1.0.tar.gz ## create myRpackage.Rcheck R CMD INSTALL myRpackage_1.0.tar.gz ## Installation in the default library Using RStudio useful keyboard shortcuts for package authoring: Build and Reload Package: Ctrl + Shift + B Check Package: Ctrl + Shift + E Test Package: Ctrl + Shift + T Using devtools: * devtools::build() * devtools::build(binary = TRUE) * devtools::check() * devtools::install() A shortcut when developing: devtools::load_all() 9.5 Package metadata The DESCRIPTION file Package: myRpackage ## mandatory (*) Type: Package ## optional, &#39;Package&#39; is default type Title: What the package does (short line) ## * Version: 1.0 ## * Date: 2013-05-10 ## release date of the current version Author: Who wrote it ## * Maintainer: Who to complain to &lt;yourfault@somewhere.net&gt; ## * Description: More about what it does (maybe more than one line) ## * License: What license is it under? ## * Depends: methods, Biostrings ## for e.g. Imports: evd ## for e.g. Suggests: BSgenome.Hsapiens.UCSC.hg19 ## for e.g. Collate: &#39;DataClasses.R&#39; &#39;read.R&#39; ## for e.g. Package dependencies: Depends A comma-separated list of package names (optionally with versions) which this package depends on. Suggests Packages that are not necessarily needed: used only in examples, tests or vignettes, loaded in the body of functions Imports Packages whose name spaces are imported from (as specified in the NAMESPACE file) which do not need to be attached to the search path. Collate Controls the collation order for the R code files in a package. If filed is present, all source files must be listed. Packages are attached to the search path with or . Attach When a package is attached, then all of its dependencies (see Depends field in its DESCRIPTION file) are also attached. Such packages are part of the evaluation environment and will be searched. Load One can also use the Imports field in the NAMESPACE file. Imported packages are loaded but are not attached: they do not appear on the search path and are available only to the package that imported them. 9.6 NAMESPACE Restricts the symbols that are exported and imports functionality from other packages. Only the exported symbols will have to be documented. export(f, g) ## exports f and g exportPattern(&quot;^[^\\\\.]&quot;) import(foo) ## imports all symbols from package foo importFrom(foo, f, g) ## imports f and g from foo It is possible to explicitely use symbol s from package foo with foo::s or foo:::s if s is not exported. 9.7 R code Contains source()able R source code to be installed. Files must start with an ASCII (lower or upper case) letter or digit and have one of the extensions .R, .S, .q, .r, or .s (use .R or .r). General style guidelines and best practice apply. Any number of files in R. Any number of functions (methods, classes) in each source file. Order matters (somehow), as the files will be sourced in the alphanumeric order. If that doesn’t fit, use the collate field in the DESCRIPTION files. Example ## works fine without Collate field AllGenerics.R DataClasses.R methods-ClassA.R methods-ClassB.R functions-ClassA.R ... zzz.R is generally used to define special functions used to initialize (called after a package is loaded and attached) and clean up (just before the package is detached). See help(&quot;.onLoad&quot;)), ?.First.Lib and ?.Last.Lib for more details. 9.8 Package sub-directories vignettes directory for vignettes in Sweave or R markdown format. data for R code, compressed tables (.tab, .txt, or .csv, see ?data for the file formats) and binary R objects. Available with data(). inst/docs for additional documentation. That’s also where the vignettes will be installed after compilation. inst/extdata directory for other data files, not belonging in data. tests code for unit tests (see here and here). src for compiled code (see the rccpp material) demo for demo code (see ?demo) 9.9 Documentation 9.10 Manual pages Package functions, datasets, methods and classes are documented in Rd, a LaTeX-like format. % File src/library/base/man/load.Rd % Part of the R package, http://www.R-project.org % Copyright 1995-2014 R Core Team % Distributed under GPL 2 or later \\name{load} \\alias{load} \\title{Reload Saved Datasets} \\description{ Reload datasets written with the function \\code{save}. } \\usage{ load(file, envir = parent.frame(), verbose = FALSE) } \\arguments{ \\item{file}{a (readable binary-mode) \\link{connection} or a character string giving the name of the file to load (when \\link{tilde expansion} is done).} \\item{envir}{the environment where the data should be loaded.} \\item{verbose}{should item names be printed during loading?} } \\details{ \\code{load} can load \\R objects saved in the current or any earlier format. It can read a compressed file (see \\code{\\link{save}}) directly from a file or from a suitable connection (including a call to \\code{\\link{url}}). [...] \\value{ A character vector of the names of objects created, invisibly. } \\section{Warning}{ Saved \\R objects are binary files, even those saved with \\code{ascii = TRUE}, so ensure that they are transferred without conversion of end of line markers. \\code{load} tries to detect such a conversion and gives an informative error message. [...] \\examples{ ## save all data xx &lt;- pi # to ensure there is some data save(list = ls(all = TRUE), file= &quot;all.RData&quot;) rm(xx) ## restore the saved values to the current environment local({ load(&quot;all.RData&quot;) ls() }) xx &lt;- exp(1:3) ## restore the saved values to the user&#39;s workspace load(&quot;all.RData&quot;) ## which is here *equivalent* to ## load(&quot;all.RData&quot;, .GlobalEnv) ## This however annihilates all objects in .GlobalEnv with the same names ! xx # no longer exp(1:3) rm(xx) attach(&quot;all.RData&quot;) # safer and will warn about masked objects w/ same name in .GlobalEnv ls(pos = 2) ## also typically need to cleanup the search path: detach(&quot;file:all.RData&quot;) ## clean up (the example): unlink(&quot;all.RData&quot;) \\dontrun{ con &lt;- url(&quot;http://some.where.net/R/data/example.rda&quot;) ## print the value to see what objects were created. print(load(con)) close(con) # url() always opens the connection }} \\keyword{file} These R documentation files can then be converted into text, pdf or html: help(&quot;load&quot;) help(&quot;load&quot;, help_type = &quot;html&quot;) help(&quot;load&quot;, help_type = &quot;pdf&quot;) One can use prompt, promptClass, promptMethods, promptPackage, promptPackage, promptData to generate Rd templates for functions, classes, methods, packages and data. 9.10.1 Use roxygen The way documentation is managed in R packages separates the code from the documentation, which makes it easier to adapt the latter when to code is updated. In comes roxygen2, that allows developer to write their documentation on top of their functions: #&#39; Reads sequences data in fasta and create \\code{DnaSeq} #&#39; and \\code{RnaSeq} instances. #&#39; #&#39; This funtion reads DNA and RNA fasta files and generates #&#39; valid \\code{&quot;DnaSeq&quot;} and \\code{&quot;RnaSeq&quot;} instances. #&#39; #&#39; @title Read fasta files. #&#39; @param infile the name of the fasta file which the data are to be read from. #&#39; @return an instance of \\code{DnaSeq} or \\code{RnaSeq}. #&#39; @seealso \\code{\\linkS4class{GenericSeq}}, \\code{\\linkS4class{DnaSeq}} and \\code{\\linkS4class{RnaSeq}}. #&#39; @examples #&#39; f &lt;- dir(system.file(&quot;extdata&quot;,package=&quot;sequences&quot;),pattern=&quot;fasta&quot;,full.names=TRUE) #&#39; f #&#39; aa &lt;- readFasta(f) #&#39; aa #&#39; @author Laurent Gatto \\email{lg390@@cam.ac.uk} #&#39; @export readFasta &lt;- function(infile){ lines &lt;- readLines(infile) header &lt;- grep(&quot;^&gt;&quot;, lines) if (length(header)&gt;1) { warning(&quot;Reading first sequence only.&quot;) lines &lt;- lines[header[1]:(header[2]-1)] header &lt;- header[1] } ##### (code cut for space reasons) ##### if (validObject(newseq)) return(newseq) } The roxygen code can then be parsed and converted to Rd using the roxygen2::roxygenise or devtools::document functions. Note that the roxygenise function does more than produce documentation (that part is handled by the rd roclet, set with roclet = &quot;rd&quot;). It can also manage your NAMESPACE file and Collate field. Note: recently, support for markdown format has been added to roxygen. 9.10.2 Vignettes While manual pages are meant to be specific and technical, vignettes are workflow-type documentation files that provide an overview and/or a use-case demonstrating the package’s functionality. Vignettes can be written in Sweave format (.Rnw extension), supporting R code chunks in LaTeX documents, or R markdown formart (.Rmd extension) for R code and markdown. The source document (.Rnw or .Rmd) can be weaved into tex or md files respectively and converted into pdf or html (Rnw to pdf only, Rmd to either) using the utils::Sweave (Rnw only) or knitr::knit functions. Rstudio makes it very easy to write and compile Rmd documents (independently of R packages). When inside a package, the documents are stored in the vignettes directory and compiled/converted automatically when the package is built. Note: if you use knitr and rmarkdown for your vignette, you’ll have to add these dependencies in the Suggests field and specify VignetteBuilder: knitr in the DESCRIPTION file. 9.11 Additional files .Rbuildignore with a list of files/dirs to ignore when building. For example the .Rproj file. .Rinstignorewith a list of files/dirs to ignore when installing. CITATION file (see citation() function) README.Rmd/README.md files if you use github. 9.12 Distributing packages CRAN Read the CRAN Repository Policy (http://cran.r-project.org/web/packages/policies.html). Upload your --as-cran checked} myPackage\\_x.y.z.tar.gz to ftp://cran.R-project.org/incoming or using http://CRAN.R-project.org/submit.html. Your package will be installable with install.packages(&quot;myRpackage&quot;). R-forge Log in, register a project and wait for acceptance. Then commit you code to the svn repository. Your package will be installable with install.packages using repos=&quot;http://R-Forge.R-project.org&quot;. GitHub (and Bitbucket) Great for development and promoting interaction and contributions. Unofficial. Autmatic checking possible through CI such as travis-ci for example. Packages can be installed with devtools::install_github (devtools::install_bitbucket). Bioconductor Make sure to satisfy submission criteria (pass check (and BiocCheck), have a vignette, use S4 if OO, make use of appropriate existing infrastructure, include a NEWS file, must not already be on CRAN, …). Your package will then be reviewed on github publicly before acceptance. A svn (git very soon) account will then be created. Package will be installable with biocLite(&quot;myPackage&quot;). "],
["visualisation-of-high-dimensional-data-in-r.html", "Chapter 10 Visualisation of high-dimensional data in R 10.1 Introduction 10.2 Data 10.3 K-means clustering 10.4 Hierarchical clustering 10.5 Principal component analysis (PCA)", " Chapter 10 Visualisation of high-dimensional data in R This section is based composed of the Visualisation of high-dimensional data presentation, and parts of the Introduction to Machine Learning with R course (unsupervised learning chapter). 10.1 Introduction Visualisation of high-dimensional data slides. 10.2 Data To illustrate some visualisation techniques, we are going to make use of Edgar Anderson’s Iris Data, available in R with data(iris) From the iris manual page: This famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. For more details, see ?iris. 10.3 K-means clustering The k-means clustering algorithms aims at partitioning n observations into a fixed number of k clusters. The algorithm will find homogeneous clusters. In R, we use stats::kmeans(x, centers = 3, nstart = 10) where x is a numeric data matrix centers is the pre-defined number of clusters the k-means algorithm has a random component and can be repeated nstart times to improve the returned model Challenge: To learn about k-means, let’s use the iris with the sepal and petal length variables only (to facilitate visualisation). Create such a data matrix and name it x Run the k-means algorithm on the newly generated data x, save the results in a new variable cl, and explore its output when printed. The actual results of the algorithms, i.e. the cluster membership can be accessed in the clusters element of the clustering result output. Use it to colour the inferred clusters to generate a figure like shown below. Figure 10.1: k-means algorithm on sepal and petal lengths i &lt;- grep(&quot;Length&quot;, names(iris)) x &lt;- iris[, i] cl &lt;- kmeans(x, 3, nstart = 10) plot(x, col = cl$cluster) 10.3.1 How does k-means work Initialisation: randomly assign class membership Figure 10.2: k-means random intialisation Iteration: Calculate the centre of each subgroup as the average position of all observations is that subgroup. Each observation is then assigned to the group of its nearest centre. It’s also possible to stop the algorithm after a certain number of iterations, or once the centres move less than a certain distance. Figure 10.3: k-means iteration: calculate centers (left) and assign new cluster membership (right) Termination: Repeat iteration until no point changes its cluster membership. k-means convergence (credit Wikipedia) 10.3.2 Model selection Due to the random initialisation, one can obtain different clustering results. When k-means is run multiple times, the best outcome, i.e. the one that generates the smallest total within cluster sum of squares (SS), is selected. The total within SS is calculated as: For each cluster results: for each observation, determine the squared euclidean distance from observation to centre of cluster sum all distances Note that this is a local minimum; there is no guarantee to obtain a global minimum. Challenge: Repeat kmeans on our x data multiple times, setting the number of iterations to 1 or greater and check whether you repeatedly obtain the same results. Try the same with random data of identical dimensions. cl1 &lt;- kmeans(x, centers = 3, nstart = 10) cl2 &lt;- kmeans(x, centers = 3, nstart = 10) table(cl1$cluster, cl2$cluster) ## ## 1 2 3 ## 1 58 0 0 ## 2 0 41 0 ## 3 0 0 51 cl1 &lt;- kmeans(x, centers = 3, nstart = 1) cl2 &lt;- kmeans(x, centers = 3, nstart = 1) table(cl1$cluster, cl2$cluster) ## ## 1 2 3 ## 1 0 0 41 ## 2 0 51 0 ## 3 58 0 0 set.seed(42) xr &lt;- matrix(rnorm(prod(dim(x))), ncol = ncol(x)) cl1 &lt;- kmeans(xr, centers = 3, nstart = 1) cl2 &lt;- kmeans(xr, centers = 3, nstart = 1) table(cl1$cluster, cl2$cluster) ## ## 1 2 3 ## 1 46 0 6 ## 2 1 51 0 ## 3 0 1 45 diffres &lt;- cl1$cluster != cl2$cluster par(mfrow = c(1, 2)) plot(xr, col = cl1$cluster, pch = ifelse(diffres, 19, 1)) plot(xr, col = cl2$cluster, pch = ifelse(diffres, 19, 1)) Figure 10.4: Different k-means results on the same (random) data 10.3.3 How to determine the number of clusters Run k-means with k=1, k=2, …, k=n Record total within SS for each value of k. Choose k at the elbow position, as illustrated below. Challenge Calculate the total within sum of squares for k from 1 to 5 for our x test data, and reproduce the figure above. ks &lt;- 1:5 tot_within_ss &lt;- sapply(ks, function(k) { cl &lt;- kmeans(x, k, nstart = 10) cl$tot.withinss }) plot(ks, tot_within_ss, type = &quot;b&quot;) 10.4 Hierarchical clustering 10.4.1 How does hierarchical clustering work Initialisation: Starts by assigning each of the n point its own cluster Iteration Find the two nearest clusters, and join them together, leading to n-1 clusters Continue merging cluster process until all are grouped into a single cluster Termination: All observations are grouped within a single cluster. Figure 10.5: Hierarchical clustering: initialisation (left) and colour-coded results after iteration (right). The results of hierarchical clustering are typically visualised along a dendrogram, where the distance between the clusters is proportional to the branch lengths. Figure 10.6: Visualisation of the hierarchical clustering results on a dendrogram In R: Calculate the distance using dist, typically the Euclidean distance. Hierarchical clustering on this distance matrix using hclust Challenge Apply hierarchical clustering on the iris data and generate a dendrogram using the dedicated plot method. d &lt;- dist(iris[, 1:4]) hcl &lt;- hclust(d) hcl ## ## Call: ## hclust(d = d) ## ## Cluster method : complete ## Distance : euclidean ## Number of objects: 150 plot(hcl) 10.4.2 Defining clusters After producing the hierarchical clustering result, we need to cut the tree (dendrogram) at a specific height to defined the clusters. For example, on our test dataset above, we could decide to cut it at a distance around 1.5, with would produce 2 clusters. Figure 10.7: Cutting the dendrogram at height 1.5. In R we can us the cutree function to cut the tree at a specific height: cutree(hcl, h = 1.5) cut the tree to get a certain number of clusters: cutree(hcl, k = 2) Challenge Cut the iris hierarchical clustering result at a height to obtain 3 clusters by setting h. Cut the iris hierarchical clustering result at a height to obtain 3 clusters by setting directly k, and verify that both provide the same results. plot(hcl) abline(h = 3.9, col = &quot;red&quot;) cutree(hcl, k = 3) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 2 3 2 3 2 3 3 3 3 2 3 2 3 3 2 3 ## [71] 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2 2 2 3 3 3 2 3 3 3 3 3 2 3 3 ## [ reached getOption(&quot;max.print&quot;) -- omitted 50 entries ] cutree(hcl, h = 3.9) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 2 3 2 3 2 3 3 3 3 2 3 2 3 3 2 3 ## [71] 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2 2 2 3 3 3 2 3 3 3 3 3 2 3 3 ## [ reached getOption(&quot;max.print&quot;) -- omitted 50 entries ] identical(cutree(hcl, k = 3), cutree(hcl, h = 3.9)) ## [1] TRUE Challenge Using the same value k = 3, verify if k-means and hierarchical clustering produce the same results on the iris data. Which one, if any, is correct? km &lt;- kmeans(iris[, 1:4], centers = 3, nstart = 10) hcl &lt;- hclust(dist(iris[, 1:4])) table(km$cluster, cutree(hcl, k = 3)) ## ## 1 2 3 ## 1 0 38 0 ## 2 50 0 0 ## 3 0 34 28 par(mfrow = c(1, 2)) plot(iris$Petal.Length, iris$Sepal.Length, col = km$cluster, main = &quot;k-means&quot;) plot(iris$Petal.Length, iris$Sepal.Length, col = cutree(hcl, k = 3), main = &quot;Hierarchical clustering&quot;) ## Checking with the labels provided with the iris data table(iris$Species, km$cluster) ## ## 1 2 3 ## setosa 0 50 0 ## versicolor 2 0 48 ## virginica 36 0 14 table(iris$Species, cutree(hcl, k = 3)) ## ## 1 2 3 ## setosa 50 0 0 ## versicolor 0 23 27 ## virginica 0 49 1 10.5 Principal component analysis (PCA) In R, we can use the prcomp function. Let’s explore PCA on the iris data. While it contains only 4 variables, is already becomes difficult to visualise the 3 groups along all these dimensions. pairs(iris[, -5], col = iris[, 5], pch = 19) Let’s use PCA to reduce the dimension. irispca &lt;- prcomp(iris[, -5]) summary(irispca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 2.0563 0.49262 0.2797 0.15439 ## Proportion of Variance 0.9246 0.05307 0.0171 0.00521 ## Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 A summary of the prcomp output shows that along PC1 along, we are able to retain over 92% of the total variability in the data. Figure 10.8: Iris data along PC1. 10.5.1 Visualisation A biplot features all original points re-mapped (rotated) along the first two PCs as well as the original features as vectors along the same PCs. Feature vectors that are in the same direction in PC space are also correlated in the original data space. biplot(irispca) One important piece of information when using PCA is the proportion of variance explained along the PCs, in particular when dealing with high dimensional data, as PC1 and PC2 (that are generally used for visualisation), might only account for an insufficient proportion of variance to be relevant on their own. In the code chunk below, I extract the standard deviations from the PCA result to calculate the variances, then obtain the percentage of and cumulative variance along the PCs. var &lt;- irispca$sdev^2 (pve &lt;- var/sum(var)) ## [1] 0.924618723 0.053066483 0.017102610 0.005212184 cumsum(pve) ## [1] 0.9246187 0.9776852 0.9947878 1.0000000 Challenge Repeat the PCA analysis on the iris dataset above, reproducing the biplot and preparing a barplot of the percentage of variance explained by each PC. It is often useful to produce custom figures using the data coordinates in PCA space, which can be accessed as x in the prcomp object. Reproduce the PCA plots below, along PC1 and PC2 and PC3 and PC4 respectively. par(mfrow = c(1, 2)) plot(irispca$x[, 1:2], col = iris$Species) plot(irispca$x[, 3:4], col = iris$Species) "],
["interactive-visualisation.html", "Chapter 11 Interactive visualisation 11.1 Interactivity graphs 11.2 Interactive apps", " Chapter 11 Interactive visualisation 11.1 Interactivity graphs This section is based on the on-line ggvis documentation The goal of ggvis is to make it easy to build interactive graphics for exploratory data analysis. ggvis has a similar underlying theory to ggplot2 (the grammar of graphics), but it’s expressed a little differently, and adds new features to make your plots interactive. ggvis also incorporates shiny’s reactive programming model (see later) and dplyr’s grammar of data transformation. library(&quot;ggvis&quot;) sml &lt;- sample(nrow(surveys), 1e3) surveys_sml &lt;- surveys_complete[sml, ] p &lt;- ggvis(surveys_sml, x = ~weight, y = ~hindfoot_length) p %&gt;% layer_points() surveys_sml %&gt;% ggvis(x = ~weight, y = ~hindfoot_length, fill = ~species_id) %&gt;% layer_points() p %&gt;% layer_points(fill = ~species_id) p %&gt;% layer_points(shape = ~species_id) To set fixed plotting parameters, use :=. p %&gt;% layer_points(fill := &quot;red&quot;, stroke := &quot;black&quot;) p %&gt;% layer_points(size := 300, opacity := 0.4) p %&gt;% layer_points(shape := &quot;cross&quot;) 11.1.1 Interactivity p %&gt;% layer_points( size := input_slider(10, 100), opacity := input_slider(0, 1)) p %&gt;% layer_points() %&gt;% add_tooltip(function(df) df$weight) input_slider() input_checkbox() input_checkboxgroup() input_numeric() input_radiobuttons() input_select() input_text() See the interactivity vignette for details. 11.1.2 Layers Simple layers layer_points(), with properties x, y, shape, stroke, fill, strokeOpacity, fillOpacity, and opacity. layer_paths(), for paths and polygons (using the fill argument). layer_ribbons() for filled areas. layer_rects(), layer_text(). Compound layers, which which combine data transformations with one or more simple layers. layer_lines() which automatically orders by the x variable with arrange(). layer_histograms() and layer_freqpolys(), which first bin the data with compute_bin(). layer_smooths(), which fits and plots a smooth model to the data using compute_smooth(). See the layers vignette for details. Like for ggplot2’s geoms, we can overly multiple layers: p %&gt;% layer_points() %&gt;% layer_smooths(stroke := &quot;red&quot;) 11.1.3 More components scales, to control the mapping between data and visual properties; see the properties and scales vignette. legends and axes to control the appearance of the guides produced by the scales. See the axes and legends vignette. Challenge Apply a PCA analysis on the iris data, and use ggvis to visualise that data along PC1 and PC2, controlling the point size using a slide bar. 11.2 Interactive apps 11.2.1 Introduction This section is based on RStudio shiny tutorials. From the shiny package website: Shiny is an R package that makes it easy to build interactive web apps straight from R. When using shiny, one tends to aim for more complete, long-lasting applications, rather then simply and transient visualisations. A shiny application is composed of a ui (user interface) and a server that exchange information using a programming paradigm called reactive programming: changes performed by the user to the ui trigger a reaction by the server and the output is updated accordingly. In the ui: define the components of the user interface (such as page layout, page title, input options and outputs), i.e what the user will see and interact with. In the server: defines the computations in the R backend. The reactive programming is implemented through reactive functions, which are functions that are only called when their respective inputs are changed. An application is run with the shiny::runApp() function, which takes the directory containing the two files as input. Before looking at the details of such an architecture, let’s build a simple example from scratch, step by step. This app, shown below, uses the faithful data, describing the wainting time between eruptions and the duration of the reuption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. head(faithful) ## eruptions waiting ## 1 3.600 79 ## 2 1.800 54 ## 3 3.333 74 ## 4 2.283 62 ## 5 4.533 85 ## 6 2.883 55 It shows the distribution of waiting times along a histogram (produced by the hist function) and provides a slider to adjust the number of bins (the breaks argument to hist). 11.2.2 Creation of our fist shiny app Create a directory that will contain the app, such as for example &quot;shinyapp&quot;. In this directory, create the ui and server files, named ui.R and server.R. In the ui.R file, let’s defines a (fluid) page containing a title panel with a page title; a layout containing a sidebar and a main panel library(shiny) shinyUI(fluidPage( titlePanel(&quot;My Shiny App&quot;), sidebarLayout( sidebarPanel( ), mainPanel( ) ) )) In the server.R file, we define the shinyServer function that handles input and ouputs (none at this stage) and the R logic. library(shiny) shinyServer(function(input, output) { }) Let’s now add some items to the ui: a text input widget in the sidebar and a field to hold the text ouput. library(shiny) shinyUI(fluidPage( titlePanel(&quot;My Shiny App&quot;), sidebarLayout( sidebarPanel( textInput(&quot;textInput&quot;, &quot;Enter text here:&quot;) ), mainPanel( textOutput(&quot;textOutput&quot;) ) ) )) In the server.R file, we add in the shinyServer function some R code defining how to manipulate the user-provided text and render it using a shiny textOuput. library(shiny) shinyServer(function(input, output) { output$textOutput &lt;- renderText(paste(&quot;User-entered text: &quot;, input$textInput)) }) Let’s now add a plot in the main panel in ui.R and some code to draw a histogram in server.R: library(shiny) shinyUI(fluidPage( titlePanel(&quot;My Shiny App&quot;), sidebarLayout( sidebarPanel( textInput(&quot;textInput&quot;, &quot;Enter text here:&quot;) ), mainPanel( textOutput(&quot;textOutput&quot;), plotOutput(&quot;distPlot&quot;) ) ) )) library(shiny) shinyServer(function(input, output) { output$textOutput &lt;- renderText(paste(&quot;User-entered text: &quot;, input$textInput)) output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] hist(x) }) }) We want to be able to control the number of breaks used to plot the histograms. We first add a sliderInput to the ui for the user to specify the number of bins, and then make use of that new input to parametrise the histogram. library(shiny) shinyUI(fluidPage( titlePanel(&quot;My Shiny App&quot;), sidebarLayout( sidebarPanel( textInput(&quot;textInput&quot;, &quot;Enter text here:&quot;), sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 30) ), mainPanel( textOutput(&quot;textOutput&quot;), plotOutput(&quot;distPlot&quot;) ) ) )) library(shiny) shinyServer(function(input, output) { output$textOutput &lt;- renderText(paste(&quot;User-entered text: &quot;, input$textInput)) output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins) }) }) The last addition is to add a menu for the user to choose a set of predefined colours (that would be a selectInput) in the ui.R file and use that new input to parametrise the colour of the histogramme in the server.R file. library(shiny) shinyUI(fluidPage( titlePanel(&quot;My Shiny App&quot;), sidebarLayout( sidebarPanel( textInput(&quot;textInput&quot;, &quot;Enter text here:&quot;), sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 30), selectInput(&quot;col&quot;, &quot;Select a colour:&quot;, choices = c(&quot;steelblue&quot;, &quot;darkgray&quot;, &quot;orange&quot;)) ), mainPanel( textOutput(&quot;textOutput&quot;), plotOutput(&quot;distPlot&quot;) ) ) )) library(shiny) shinyServer(function(input, output) { output$textOutput &lt;- renderText(paste(&quot;User-entered text: &quot;, input$textInput)) output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins, col = input$col) }) }) The last addition that we want is to visualise the actual data in the main panel. We add a dataTableOutput in ui.R and generate that table in server.R using a renderDataTable rendering function. library(shiny) ## Define UI for application that draws a histogram shinyUI(fluidPage( ## Application title titlePanel(&quot;My Shiny App&quot;), ## Sidebar with text, slide bar and menu selection inputs sidebarLayout( sidebarPanel( textInput(&quot;textInput&quot;, &quot;Enter text here:&quot;), sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 30), selectInput(&quot;col&quot;, &quot;Select a colour:&quot;, choices = c(&quot;steelblue&quot;, &quot;darkgray&quot;, &quot;orange&quot;)) ), ## Main panel showing user-entered text, a reactive plot and a ## dynamic table mainPanel( textOutput(&quot;textOutput&quot;), plotOutput(&quot;distPlot&quot;), dataTableOutput(&quot;dataTable&quot;) ) ) )) library(shiny) ## Define server logic shinyServer(function(input, output) { output$textOutput &lt;- renderText(paste(&quot;User-entered text: &quot;, input$textInput)) ## Expression that generates a histogram. The expression is ## wrapped in a call to renderPlot to indicate that: ## ## 1) It is &quot;reactive&quot; and therefore should be automatically ## re-executed when inputs change ## 2) Its output type is a plot output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] ## Old Faithful Geyser data bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) ## draw the histogram with the specified number of bins hist(x, breaks = bins, col = input$col, border = &#39;white&#39;) }) output$dataTable &lt;- renderDataTable(faithful) }) Challenge Write and your shinyapp applications, as described above. 11.2.3 The shiny infrastructure The overview figure below is based and makes reference to lessons of the written tutorial. 11.2.4 Another shiny app The ui uses the fluidPage UI and a sidebar layout. The sidebar panel contains a textInput with a caption, a selectInput to choose from the three possible datasets rock, pressure or cars, and a numericInput to define how many observations to show. The main panel use a textOutput to display the caption, a verbatimOutput to show the output of the summary function on the data chosen in the selectInput above, and a tableOutput to show the head of that same data. library(&quot;shiny&quot;) # Define UI for dataset viewer application shinyUI(fluidPage( # Application title titlePanel(&quot;Reactivity&quot;), # Sidebar with controls to provide a caption, select a dataset, # and specify the number of observations to view. Note that # changes made to the caption in the textInput control are # updated in the output area immediately as you type sidebarLayout( sidebarPanel( textInput(&quot;caption&quot;, &quot;Caption:&quot;, &quot;Data Summary&quot;), selectInput(&quot;dataset&quot;, &quot;Choose a dataset:&quot;, choices = c(&quot;rock&quot;, &quot;pressure&quot;, &quot;cars&quot;)), numericInput(&quot;obs&quot;, &quot;Number of observations to view:&quot;, 10) ), # Show the caption, a summary of the dataset and an HTML # table with the requested number of observations mainPanel( h3(textOutput(&quot;caption&quot;, container = span)), verbatimTextOutput(&quot;summary&quot;), tableOutput(&quot;view&quot;) ) ) )) The server defines a reactive expression that sets the appropriate data based on the selectInput above. It produces three outputs: the caption using renderText and the caption defined above; the appropriate summary using renderPrint and the reactive data; the table using renderTable to produce the head with the reactive data and number of observations defined by the numbericInput above. library(shiny) library(datasets) # Define server logic required to summarize and view the selected # dataset shinyServer(function(input, output) { # By declaring datasetInput as a reactive expression we ensure # that: # # 1) It is only called when the inputs it depends on changes # 2) The computation and result are shared by all the callers # (it only executes a single time) # datasetInput &lt;- reactive({ switch(input$dataset, &quot;rock&quot; = rock, &quot;pressure&quot; = pressure, &quot;cars&quot; = cars) }) # The output$caption is computed based on a reactive expression # that returns input$caption. When the user changes the # &quot;caption&quot; field: # # 1) This function is automatically called to recompute the # output # 2) The new caption is pushed back to the browser for # re-display # # Note that because the data-oriented reactive expressions # below don&#39;t depend on input$caption, those expressions are # NOT called when input$caption changes. output$caption &lt;- renderText({ input$caption }) # The output$summary depends on the datasetInput reactive # expression, so will be re-executed whenever datasetInput is # invalidated # (i.e. whenever the input$dataset changes) output$summary &lt;- renderPrint({ dataset &lt;- datasetInput() summary(dataset) }) # The output$view depends on both the databaseInput reactive # expression and input$obs, so will be re-executed whenever # input$dataset or input$obs is changed. output$view &lt;- renderTable({ head(datasetInput(), n = input$obs) }) }) Challenge Using the code above, implement and run the app. 11.2.5 Single-file app Instead of defining the ui and server in their respective files, they can be combined into list to be passed directly to runApp: ui &lt;- fluidPage(...) server &lt;- function(input, output) { ... } app &lt;- list(ui = ui, server = server) runApp(app) Challenges Create an app to visualise the iris data where the user can select along which features to view the data. As above, where the visualisation is a PCA plot and the user chooses the PCs. 11.2.6 There’s more to shiny 11.2.6.1 Sharing shiny apps Share the code file(s) and runApp runUrl runGitHub runGist shinyapps Shiny server (in-house) 11.2.6.2 More interactivity plotOutput(&quot;pca&quot;, hover = &quot;hover&quot;, click = &quot;click&quot;, dblclick = &quot;dblClick&quot;, brush = brushOpts( id = &quot;brush&quot;, resetOnNew = TRUE)) Example here. 11.2.6.3 References shiny page shiny cheat sheet "],
["navigating-the-bioconductor-project.html", "Chapter 12 Navigating the Bioconductor project 12.1 biocViews 12.2 Workflows 12.3 Learning about specific packages 12.4 Versions 12.5 Getting help 12.6 Data infrastructure 12.7 More resources 12.8 Challenges", " Chapter 12 Navigating the Bioconductor project 12.1 biocViews Bioconductor has become a large project proposing many packages (1473 software packages at the time of writing) across many domains of high throughput biology. It continues to grow, at an increasing rate, and it can be difficult to get started. One way to find packages of interest is to navigate the biocViews hierarchy. Every package is tagged with a set of biocViews labels. The highest level defines 3 types of packages: Software: packages providing a specific functionality. AnnotationData: packages providing annotations, such as various ontologies, species annotations, microarray annotations, … ExperimentData: packages distributing experiments. The biocViews page is available here https://bioconductor.org/packages/release/BiocViews.html#___Software It is most easily accessed by clicking on the software packages link on the homepage, under About Bioconductor. See also this page for additional information. 12.2 Workflows On the other hand, people generally don’t approach the Bioconductor project to learn the whole project, but are interested by a specific analysis from a Bioconductor package, that they have read in a paper of interest. In my opinion, it is more effective to restrict ones attention to a problem or analysis of interest to first immerse oneself into Bioconductor, then broaden up ones experience to other topics and packages. To to that, the project offers workflows that provide a general introduction to topics such as sequence analysis, annotation resources, RNA-Seq data analyis, Mass spectrometry and proteomics, CyTOF analysis, …. https://bioconductor.org/help/workflows/ A similar set of resources are published in F1000Research under the Bioconductor gateway https://f1000research.com/gateways/bioconductor These peer-reviewed papers describe more complete pipelines involving one or several packages. 12.3 Learning about specific packages Each Bioconductor package has it’s own landing pages that provides all necessary information about a package, including a short summary, its current version, the authors, how the cite the package, installation instructions, and links to all package vignettes. Any Bioconductor package page can be contructed by appending the package’s name to https://bioconductor.org/packages/ to produce an URL like https://bioconductor.org/packages/packagename This works for any type of package (software, annotation or data). For example, the pages for packages DESeq2 or MSnbase would be https://bioconductor.org/packages/DESeq2 and https://bioconductor.org/packages/MSnbase These short URLs are then resolved to their longer form to redirect to the longer package URL leading the user to the current release version of the packge. 12.3.1 Package vignettes An important quality of every Bioconductor package is the availability of a dedicated vignette. Vignettes are documentations (generally provided as pdf or html files) that provide a generic overview of the package, without necessarily going in detail for every function of the package. Vignettes are special in that respect as they are produced as part of the package building process. The code in a vignette is executed and its output, whether in the form of simple text, tables and figures, are inserted in the vignette before the final file (in pdf or html) is produced. Hence, all the code and outputs are guaranteed to be correct and reproduced. Given a vignette, it is this possible to re-generate all the results. To make reproducing a long vignette as easy as possible without copy and pasting all code chunks one by one, it is possible to extract the code into an R script runnung the Stangle (from the utils package - see here for details) or knitr::purl functions on the vignette source document. 12.3.2 Installation The installation of a Bioconductor package is done using the biocLite function from the BiocInstaller package. The first time, when the package isn’t available yet, you need to run ## try http:// if https:// URLs are not supported source(&quot;https://bioconductor.org/biocLite.R&quot;) The above command will install and load BiocInstaller. If it isn’t loaded yet, first load it with library, then install your package(s) of interest with biocLite: library(&quot;BiocInstaller&quot;) biocLite(&quot;Biobase&quot;) This page provides more details on package installation and update. 12.4 Versions It is also useful to know that at any given time, there are two Bioconductor versions - there is always a release (stable) and a development (devel) versions. For example, in October 2017, the release version is 3.6 and the development is 3.7. The individual packages have a similar scheme. Every package is available for the release and the development versions of Bioconductor. These two versions of the package have also different version numbers, where the last digit is even for the former and off for the later. Currently, the MSnbase has versions 2.4.0 and 2.5.1, respectively. Finally, every Bioconductor version is tight to an R version. To access the current Bioconductor release 3.6 version, one needs to use the latest R version, which is 3.4.2. Hence, it is important to have an up-to-date R installation to keep up with the latest developments in Bioconductor. More details here. 12.5 Getting help The best way to get help with regard the a Bioconductor package is to post the question on the Bioconductor support forum at https://support.bioconductor.org/. Package developers generally follow the support site for questions related to their packages. See this page for some details. To maximise the chances to obtain a answer promptly, it is important to provide details for other to understand the question and, if relevant, reproduce the observed errors. The Bioconductor project has a dedicated posting guide. Here’s another useful guide on how to write a reproducible question. Packages come with a lot of documentation build in, that users are advised to read to familiarise themselves with the package and how to use it. In addition to the package vignettes are describe above, every function of class in a package is documented in great detail in their respective man page, that can be accessed with ?function. There is also a dedicated developer mailing list that is dedicated for questions and discussions related to package development. 12.6 Data infrastructure An essential aspect that is central to Bioconductor and its success is the availability of core data infrastructure that is used across packages. Package developers are advised to make use of existing infrastructure to provide coherence, interoperability and stability to the project as a whole. Here are some core classes, taken from the Common Bioconductor Methods and Classes page: Importing GTF, GFF, BED, BigWig, etc., - rtracklayer::import() VCF – VariantAnnotation::readVcf() SAM / BAM – Rsamtools::scanBam(), GenomicAlignments:readGAlignment*() FASTA – Biostrings::readDNAStringSet() FASTQ – ShortRead::readFastq() Mass spectrometry data (XML-based and mfg formats) – MSnbase::readMSData(), MSnbase::readMgfData() Common Classes Rectangular feature x sample data – SummarizedExperiment::SummarizedExperiment() (RNAseq count matrix, microarray, …) Genomic coordinates – GenomicRanges::GRanges() (1-based, closed interval) DNA / RNA / AA sequences – Biostrings::*StringSet() Gene sets – GSEABase::GeneSet() GSEABase::GeneSetCollection() Multi-omics data – MultiAssayExperiment::MultiAssayExperiment() Single cell data – SingleCellExperiment::SingleCellExperiment() Mass spectrometry data – MSnbase::MSnExp() A common design theme that is found throughout many Bioconductor core classes is illustrated below, which is found in microarrays, quantiative proteomics data, RNA-Seq data, … It contains a rectangular feature x sample data matrix and sample and feature-specific annotations. 12.7 More resources Videos: https://www.youtube.com/user/bioconductor Community Contributed Help Resources: https://bioconductor.org/help/community/ Tutorials: https://support.bioconductor.org/t/Tutorials/ 12.8 Challenges Install a Bioconductor package of your choice, discover the vignette(s) it offers, open one, and extract the R code of it. Find a package that allows reading raw mass spectrometry data and identify the specific function. Either use the biocViews tree, look for a possible workflow, or look in the common methods and classes page on the Bioconductor page. "],
["r-programming-concepts-and-tools.html", "Chapter 13 R programming concepts and tools 13.1 Defensive programming 13.2 KISS 13.3 Failing fast and well 13.4 Consistency and predictability 13.5 Comparisons 13.6 Exercise 13.7 Debugging: techniques and tools", " Chapter 13 R programming concepts and tools This section is composed of various section of more advanced programming topics from the Teaching Material page. 13.1 Defensive programming Before even debugging, let’s look at ways to prevent bugs in the first place. Defensive programming: making the code work in a predicable manner writing code that fails in a well-defined manner if something weird happens, either properly deal with it, of fail quickly and loudly The level of defensiveness will depend whether you write a function for interactive of programmatic usage. Talking to users Diagnostic messages message(&quot;This is a message for our dear users.&quot;) message(&quot;This is a message for our dear users. &quot;, paste(&quot;Thank you for using our software&quot;, sw, &quot;version&quot;, packageVersion(sw))) Do not use print or cat: f1 &lt;- function() { cat(&quot;I AM LOUD AND YOU CAN&#39;T HELP IT.\\n&quot;) ## do stuff invisible(TRUE) } f1() f2 &lt;- function() { message(&quot;Sorry to interup, but...&quot;) ## do stuff invisible(TRUE) } f2() suppressMessages(f2()) Of course, it is also possible to manually define verbosity. This makes you write more code for a feature readily available. But still better to use message. f3 &lt;- function(verbose = TRUE) { if (verbose) message(&quot;I am being verbose because you let me.&quot;) ## do stuff invisible(TRUE) } f3() f3(verbose = FALSE) Warning There is a problem with warnings. No one reads them. Pat Burns, in R inferno. warning(&quot;Do not ignore me. Somthing bad might have happened.&quot;) warning(&quot;Do not ignore me. Somthing bad might be happening.&quot;, immediate. = TRUE) f &lt;- function(...) warning(&quot;Attention, attention, ...!&quot;, ...) f() f(call. = FALSE) Print warnings after they have been thrown. warnings() last.warning See also to warn option in ?options . option(&quot;warn&quot;) Error stop(&quot;This is the end, my friend.&quot;) log(c(2, 1, 0, -1, 2)); print(&#39;end&#39;) ## warning xor(c(TRUE, FALSE)); print (&#39;end&#39;) ## error Stop also has a call. parameter. geterrmessage() Progress bars utils::txtProgressBar function n &lt;- 10 pb &lt;- txtProgressBar(min = 0, max = n, style = 3) for (i in 1:n) { setTxtProgressBar(pb, i) Sys.sleep(0.5) } close(pb) progress package library(&quot;progress&quot;) pb &lt;- progress_bar$new(total = n) for (i in 1:n) { pb$tick() Sys.sleep(0.5) } Tip: do not over use progress bars. Ideally, a user should be confident that everything is under control and progress is made while waiting for a function to return. In my experience, a progress bar is usefull when there is a specific and/or user-defined number of iterations, such a iterating over n files, or running a simulation n times. 13.2 KISS Keep your functions simple and stupid (and short). 13.3 Failing fast and well Bounds errors are ugly, nasty things that should be stamped out whenever possible. One solution to this problem is to use the assert statement. The assert statement tells C++, “This can never happen, but if it does, abort the program in a nice way.” One thing you find out as you gain programming experience is that things that can “never happen” happen with alarming frequency. So just to make sure that things work as they are supposed to, it’s a good idea to put lots of self checks in your program. – Practical C++ Programming, Steve Oualline, O’Reilly. if (!condition) stop(...) stopifnot(TRUE) stopifnot(TRUE, FALSE) For example to test input classes, lengths, … f &lt;- function(x) { stopifnot(is.numeric(x), length(x) == 1) invisible(TRUE) } f(1) f(&quot;1&quot;) f(1:2) f(letters) The assertthat package: x &lt;- &quot;1&quot; library(&quot;assertthat&quot;) stopifnot(is.numeric(x)) assert_that(is.numeric(x)) assert_that(length(x) == 2) assert_that() signal an error. see_if() returns a logical value, with the error message as an attribute. validate_that() returns TRUE on success, otherwise returns the error as a string. is.flag(x): is x TRUE or FALSE? (a boolean flag) is.string(x): is x a length 1 character vector? has_name(x, nm), x %has_name% nm: does x have component nm? has_attr(x, attr), x %has_attr% attr: does x have attribute attr? is.count(x): is x a single positive integer? are_equal(x, y): are x and y equal? not_empty(x): are all dimensions of x greater than 0? noNA(x): is x free from missing values? is.dir(path): is path a directory? is.writeable(path)/is.readable(path): is path writeable/readable? has_extension(path, extension): does file have given extension? 13.4 Consistency and predictability Ineractive use vs programming: Moving from using R to programming R is abstraction, automation, generalisation. drop head(cars) head(cars[, 1]) head(cars[, 1, drop = FALSE]) sapply/lapply df1 &lt;- data.frame(x = 1:3, y = LETTERS[1:3]) sapply(df1, class) df2 &lt;- data.frame(x = 1:3, y = Sys.time() + 1:3) sapply(df2, class) Rather use a form where the return data structure is known… lapply(df1, class) lapply(df2, class) or that will break if the result is not what is exected vapply(df1, class, &quot;1&quot;) vapply(df2, class, &quot;1&quot;) Reminder of the interactive use vs programming examples: - [ and drop - sapply, lapply, vapply Remember also the concept of tidy data. 13.5 Comparisons Floating point issues to be aware of R FAQ 7.31? a &lt;- sqrt(2) a * a == 2 a * a - 2 1L + 2L == 3L 1.0 + 2.0 == 3.0 0.1 + 0.2 == 0.3 Floating point: how to compare all.equal compares R objects for near equality. Takes into account whether object attributes and names ought the taken into consideration (check.attributes and check.names parameters) and tolerance, which is machine dependent. all.equal(0.1 + 0.2, 0.3) all.equal(0.1 + 0.2, 3.0) isTRUE(all.equal(0.1 + 0.2, 3)) ## when you just want TRUE/FALSE Exact identity identical: test objects for exact equality 1 == NULL all.equal(1, NULL) identical(1, NULL) identical(1, 1.) ## TRUE in R (both are stored as doubles) all.equal(1, 1L) identical(1, 1L) ## stored as different types Appropriate within if, while condition statements. (not all.equal, unless wrapped in isTRUE). 13.6 Exercise From Advanced R by Hadley Wickham. The col_means function computes the means of all numeric columns in a data frame. col_means &lt;- function(df) { numeric &lt;- sapply(df, is.numeric) numeric_cols &lt;- df[, numeric] data.frame(lapply(numeric_cols, mean)) } Is it a robust function? What happens if there are unusual inputs. col_means(mtcars) col_means(mtcars[, 0]) col_means(mtcars[0, ]) col_means(mtcars[, &quot;mpg&quot;, drop = FALSE]) col_means(1:10) col_means(as.matrix(mtcars)) col_means(as.list(mtcars)) mtcars2 &lt;- mtcars mtcars2[-1] &lt;- lapply(mtcars2[-1], as.character) col_means(mtcars2) Using some of the concepts and tips above, re-write col_means to make it more robust. 13.7 Debugging: techniques and tools Shit happens! Funding your bug is a process of confirming the many things that you believe are true - until you find one which is not true. – Norm Matloff 1. Identify the bug (the difficult part) Something went wrong! Where in the code does it happen? Does it happen every time? What input triggered it? Report it (even if it is in your code - use github issues, for example). Tip: Beware of your intuition. As a scientist, do what you are used to: generate a hypotheses, design an experiment to test them, and record the results. 2. Fix it (the less difficult part) Correct the bug. Make sure that bug will not repeat itself! How can we be confident that we haven’t introduced new bugs? Tools print/cat traceback() browser() IDE: RStudio, StatET, emacs’ ess tracebug. Manually Inserting print and cat statements in the code. Works, but time consuming. Finding the bug Bugs are shy, and are generally hidden, deep down in your code, to make it as difficult as possible for you to find them. e &lt;- function(i) { x &lt;- 1:4 if (i &lt; 5) x[1:2] else x[-1:2] } f &lt;- function() sapply(1:10, e) g &lt;- function() f() traceback: lists the sequence of calls that lead to the error g() traceback() If the source code is available (for example for source()d code), then traceback will display the exact location in the function, in the form filename.R#linenum. Browsing the error Register the function for debugging: debug(g). This adds a call to the browser() function (see also below) and the very beginning of the function g. Every call to g() will not be run interactively. To finish debugging: undebug(g). debug(g) g() How to debug: n executes the next step of the function. Use print(n) or get(n) to print/access the variable n. s to step into the next function. If it is not a function, same as n. f to finish execution of the current loop of function. c to leave interactive debugging and continue regular execution of the function. Q to stop debugging, terminate the function and return to the global workspace. where print a stack trace of all active function calls. Enter same as n (or s, if it was used most recently), unless options(browserNLdisabled = TRUE) is set. To fix a function when the source code is not directly available, use fix(fun). This will open the function’s source code for editing and, after saving and closing, store the updated function in the global workspace. Breakpoints Add a call to browser() anywhere in the source code to execute the rest of the code interactively. To run breakpoints conditionally, wrap the call to browser() in a condition. Debugging with IDEs RSudio: Show Traceback, Rerun with Debug and interactive debugging. StatET (Eclipse plugin) emacs ESS and tracebug Exercise Your turn - play with traceback, recover and debug: (Example originally by Martin Morgan and Robert Gentleman.) e &lt;- function(i) { x &lt;- 1:4 if (i &lt; 5) x[1:2] else x[-1:2] # oops! x[-(1:2)] } f &lt;- function() sapply(1:10, e) g &lt;- function() f() Fix readFasta2. Preparing the ground ## make sure you have the &#39;sequences&#39; package. library(&quot;devtools&quot;) install_github(&quot;lgatto/sequences&quot;) ## from github ## or install.packages(&quot;sequences&quot;) ## from CRAN A working example: reading a single sequence from a fasta file to create a object of class DnaSeq, representing the DNA string: library(&quot;sequences&quot;) ## Loading required package: Rcpp ## This is package &#39;sequences&#39; ## ## Attaching package: &#39;sequences&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## id f &lt;- dir(system.file(&quot;extdata&quot;, package = &quot;sequences&quot;), full.names=TRUE, pattern = &quot;aDnaSeq.fasta&quot;) readFasta(f) ## Object of class DnaSeq ## Id: example dna sequence ## Length: 132 ## Alphabet: A C G T ## Sequence: AGCATACGACGACTACGACACTACGACATCAGACACTACAGACTACTACGACTACAGACATCAGACACTACATATTTACATCATCAGAGATTATATTAACATCAGACATCGACACATCATCATCAGCATCAT A bug, trying to read multiple sequences from a fasta file. The expected behaviour would be to return a list of DnaSeq objects: ## Get readFasta2, the function to debug sequences:::debugme() ## Get an example file f &lt;- dir(system.file(&quot;extdata&quot;, package = &quot;sequences&quot;), full.names=TRUE, pattern = &quot;moreDnaSeqs.fasta&quot;) ## BANG! readFasta2(f) "],
["session-information.html", "Session information", " Session information The version of R and all packages used to generate this material are documented below. sessionInfo() ## R version 3.4.2 Patched (2017-10-03 r73455) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.5 LTS ## ## Matrix products: default ## BLAS: /usr/lib/atlas-base/libf77blas.so.3.0 ## LAPACK: /usr/lib/lapack/liblapack.so.3.0 ## ## locale: ## [1] LC_CTYPE=en_GB.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_GB.UTF-8 LC_COLLATE=en_GB.UTF-8 ## [5] LC_MONETARY=en_GB.UTF-8 LC_MESSAGES=en_GB.UTF-8 ## [7] LC_PAPER=en_GB.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] methods stats graphics grDevices utils datasets base ## ## other attached packages: ## [1] sequences_0.5.9 Rcpp_0.12.13 roxygen2_6.0.1 devtools_1.13.3 ## [5] gridExtra_2.3 bindrcpp_0.2 purrr_0.2.4 readr_1.1.1 ## [9] tidyr_0.7.2 tibble_1.3.4 tidyverse_1.1.1 lubridate_1.6.0 ## [13] shiny_1.0.5 ggvis_0.4.3 ggplot2_2.2.1 dplyr_0.7.4 ## [17] BiocStyle_2.5.43 knitr_1.17 DT_0.2 ## ## loaded via a namespace (and not attached): ## [1] msdata_0.17.1 lattice_0.20-35 assertthat_0.2.0 rprojroot_1.2 ## [5] digest_0.6.12 psych_1.7.8 mime_0.5 R6_2.2.2 ## [9] cellranger_1.1.0 plyr_1.8.4 backports_1.1.1 evaluate_0.10.1 ## [13] httr_1.3.1 highr_0.6 rlang_0.1.2 lazyeval_0.2.0 ## [17] readxl_1.0.0 rstudioapi_0.7 rmarkdown_1.6 labeling_0.3 ## [21] stringr_1.2.0 foreign_0.8-69 htmlwidgets_0.9 munsell_0.4.3 ## [25] broom_0.4.2 compiler_3.4.2 httpuv_1.3.5 modelr_0.1.1 ## [29] pkgconfig_2.0.1 mnormt_1.5-5 htmltools_0.3.6 tidyselect_0.2.2 ## [33] bookdown_0.5 withr_2.0.0 commonmark_1.4 grid_3.4.2 ## [37] nlme_3.1-131 jsonlite_1.5 xtable_1.8-2 gtable_0.2.0 ## [41] magrittr_1.5 scales_0.5.0 stringi_1.1.5 reshape2_1.4.2 ## [45] xml2_1.1.1 tools_3.4.2 forcats_0.2.0 glue_1.2.0 ## [49] hms_0.3 parallel_3.4.2 yaml_2.1.14 colorspace_1.3-2 ## [53] rvest_0.3.2 memoise_1.1.0 bindr_0.1 haven_1.1.0 "]
]
